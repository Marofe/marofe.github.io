(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{10:function(e,a,t){},17:function(e,a,t){},32:function(e,a,t){e.exports=t(55)},37:function(e,a,t){},38:function(e,a,t){},46:function(e,a,t){},47:function(e,a,t){},49:function(e,a,t){},55:function(e,a,t){"use strict";t.r(a);var n=t(0),i=t.n(n),r=t(25),l=t.n(r),o=(t(37),t(38),function(e){return i.a.createElement("div",{className:"layout"},e.children)}),s=t(14),m=t(2),c=t(3),d=t.n(c),h=(t(46),function(){return i.a.createElement("div",{className:"divPage"},i.a.createElement(d.a,null,i.a.createElement("title",null,"Marcos Rog\xe9rio Fernandes | Personal Website"),i.a.createElement("meta",{name:"description",content:"Welcome to my personal website. Here you will find my reserach interest and contributions. "})),i.a.createElement("div",{className:"homepage_top"},i.a.createElement("img",{alt:"perfil",class:"homepage_myfoto",src:"https://avatars1.githubusercontent.com/u/4412144?s=460&v=4"}),i.a.createElement("h1",null,"Marcos Rog\xe9rio Fernandes"),i.a.createElement("p",null,"I'm a Control Engineer, programmer and a researcher. I work on ideas and tools related to Electrical Engineer, Automation and Computer Science. I'm a PhD Candidate at School of Electrical and Computer Engineering from Unicamp (FEEC/Unicamp).")),i.a.createElement("article",null,i.a.createElement("h2",null,"My Research Interest:"),i.a.createElement("ul",null,i.a.createElement("li",null,"Statistic Learning"),i.a.createElement("li",null,"Kalman Filtering"),i.a.createElement("li",null,"Optimal and Robust Stochastic Control Systems"),i.a.createElement("li",null,"Navigation & Tracking Systems"),i.a.createElement("li",null,"Sensor Fusion Algorithms"),i.a.createElement("li",null,"GNSS Processing Methods"),i.a.createElement("li",null,"Mobile Robotics"),i.a.createElement("li",null,"Computer Vision")),i.a.createElement("div",{className:"social_media"},i.a.createElement("a",{href:"https://www.instagram.com/_marofe",class:"fa fa-instagram"}),i.a.createElement("a",{href:"https://www.linkedin.com/in/marcos-rogerio-fernandes/",class:"fa fa-linkedin"}),i.a.createElement("a",{href:"https://www.researchgate.net/profile/Marcos_Fernandes10",class:"fa fa-researchgate"}),i.a.createElement("a",{href:"https://github.com/Marofe",class:"fa fa-github"}),i.a.createElement("a",{href:"mailto:eng.marofe@hotmail.com",class:"fa fa-envelope"}))))}),u=(t(47),function(){return i.a.createElement("div",{className:"divPages pgPublication"},i.a.createElement(d.a,null,i.a.createElement("title",null,"Publications | Marcos Rog\xe9rio Fernandes"),i.a.createElement("meta",{name:"description",content:"Welcome to my personal website. Here you will find my main contributions. "})),i.a.createElement("div",{className:"top"},i.a.createElement("h1",null,"My Publications"),i.a.createElement("p",null,"Here is an overview of my latest works and publications.")),i.a.createElement("article",null,i.a.createElement("h2",null,"Journals:"),i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("p",null,'M. R. Fernandes, J. B. R. do Val and R. F. Souto, "Robust Estimation and Filtering for Poorly Known Models," in IEEE Control Systems Letters, vol. 4, no. 2, pp. 474-479, April 2020.',i.a.createElement("br",null),"[",i.a.createElement("a",{href:"https://ieeexplore.ieee.org/document/8891731"},"Download (IEEE Explorer)"),"]"))),i.a.createElement("h2",null,"Conference Papers:"),i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("p",null,"FERNANDES, MARCOS R.; DO VAL, JOAO B. R. ; SOUTO, RAFAEL F. . Filtering of Poorly Known Systems: Estimation Variations as Source of Uncertainty. In: 2018 IEEE Conference on Decision and Control (CDC), 2018, FL. 2018 IEEE Conference on Decision and Control (CDC), 2018. p. 3074.",i.a.createElement("br",null),"[",i.a.createElement("a",{href:"https://ieeexplore.ieee.org/document/8619306"},"Download (IEEE Explorer)"),"][",i.a.createElement("a",{href:"https://www.researchgate.net/publication/329895979_Slides"},"Slides"),"] [",i.a.createElement("a",{href:"https://github.com/Marofe/EVIU"},"Code"),"]")),i.a.createElement("li",null,i.a.createElement("p",null,"FERNANDES, M. R.; SOUTO, R. F. ; DO VAL, J. B. R. . FILTRAGEM DE SISTEMAS NA\u0303O-LINEARES: CONSIDERANDO A VARIAC\u0327A\u0303O DA ESTIMATIVA COMO FONTE DE INCERTEZA. In: Congresso Brasileiro de Autom\xe1tica, 2018, Jo\xe3o Pessoa. Anais do XXII Congresso Brasileiro de Autom\xe1tica, 2018.",i.a.createElement("br",null),"[",i.a.createElement("a",{href:"http://dx.doi.org/10.20906/CPS/CBA2018-1140"},"Download (Portuguese)"),"][",i.a.createElement("a",{href:"https://github.com/Marofe/EVIU"},"Code"),"]")),i.a.createElement("li",null,i.a.createElement("p",null,"DE OLIVEIRA, MARIO. O. F. ; FERNANDES, M. R. ; SOUTO, RAFAEL F. . Implementation of a Low-cost Prototype of Twin Rotor for academic studies in identification, optimal control and stochastic filtering. In: 2017 6th International Conference on Systems and Control (ICSC), 2017, Batna. 2017 6th International Conference on Systems and Control (ICSC), 2017. p. 193-198.",i.a.createElement("br",null),"[",i.a.createElement("a",{href:"https://ieeexplore.ieee.org/document/7958718"},"Download (IEEE Explorer)"),"]")),i.a.createElement("li",null,i.a.createElement("p",null,"FERNANDES, MARCOS. R.; DE OLIVEIRA, MARIO. O. F. ; SOUTO, R. F. . CONSTRU\xc7\xc3O DE UM PROT\xd3TIPO DE HELIC\xd3PTERO DE BAIXO CUSTO PARA ESTUDOS EM IDENTIFICA\xc7\xc3O DE SISTEMAS. In: Simp\xf3sio Brasileiro de Automa\xe7\xe3o Inteligente, 2017, Porto Alegre. Anais do XIII Simpo\u0301sio Brasileiro de Automac\u0327a\u0303o Inteligente, 2017. p. 1177-1183.",i.a.createElement("br",null),"[",i.a.createElement("a",{href:"https://www.ufrgs.br/sbai17/papers/paper_332.pdf"},"Download (Portuguese)"),"]"))),i.a.createElement("h2",null,"Master's Dissertation:"),i.a.createElement("p",null,"FERNANDES, M. R.; Stochastic Filtering: Estimation Variation as Source of Uncertainty. FEEC/UNICAMP, 2019.",i.a.createElement("br",null),"[",i.a.createElement("a",{href:"http://repositorio.unicamp.br/jspui/handle/REPOSIP/334481"},"Download (Portuguese)"),"] [",i.a.createElement("a",{href:"https://www.researchgate.net/publication/334710395_mestrado-slidespdf"},"Slides"),"] [",i.a.createElement("a",{href:"https://github.com/Marofe/EVIU"},"Code"),"]"),i.a.createElement("h2",null,"Undergraduate's Final Project:"),i.a.createElement("p",null,"FERNANDES, M. R.; DE OLIVEIRA, M. O. F. ; Study and Development of Optimal Control Systems and Stochastic Filtering. UTFPR, 2016.",i.a.createElement("br",null),"[",i.a.createElement("a",{href:"https://www.researchgate.net/publication/313426875_Estudo_e_Desenvolvimento_de_Sistemas_de_Controle_Otimo_com_Filtragem_Estocastica"},"Download (Portuguese)"),"]")))}),p=(t(10),t(1)),g=t(13),f=t.n(g),b=function(e){return i.a.createElement("span",{dangerouslySetInnerHTML:{__html:f.a.renderToString(e.math)}})},E=(t(12),t(49),function(e){return i.a.createElement("div",{className:"divImage"},i.a.createElement("img",{className:e.className,alt:e.alt,src:e.src,width:e.width,height:e.height}),i.a.createElement("br",null),i.a.createElement("span",null,e.legend))}),_=t(6),k=t.n(_),x=function(){return console.log("/tutorials/rastreamento_usando_visao_filtro_kalman"),i.a.createElement("article",null,i.a.createElement(d.a,null,i.a.createElement("title",null,"Rastreamento de Objetos usando Vis\xe3o Computacional e Filtro de Kalman | Marcos Rog\xe9rio Fernandes"),i.a.createElement("meta",{name:"description",content:"Welcome to my personal website. Here you will find my reserach interest and contributions. "})),i.a.createElement("h1",null,"Rastreamento de Objetos usando Vis\xe3o Computacional e Filtro de Kalman"),i.a.createElement(E,{src:"https://4.bp.blogspot.com/-J-tkGAP5-DA/WV7cvNDr5mI/AAAAAAAAAV4/hHQS3uj5H-gK040plU-Ikg6MSpQjtSmDwCLcBGAs/s400/Untitled.jpg",alt:"Computer Vision",legend:"Rastreamento de Objetos"}),i.a.createElement("p",null,"Este trabalho apresenta um algoritmo em tempo real para rastreamento de objetos em vis\xe3o computacional, usando o Filtro de Kalman como mecanismo de predi\xe7\xe3o para situa\xe7\xf5es de oclus\xe3o e ou contamina\xe7\xe3o da cena por ru\xeddo. O principal objetivo deste trabalho \xe9 de apresentar de forma did\xe1tica o desenvolvimento de um algoritmo de rastreamento de objetos baseado em cor. O algoritmo apresentado faz o rastreamento do maior objeto sim\xe9trico de uma cor pr\xe9-definida presente na cena. \xc9 apresentado em detalhes a implementa\xe7\xe3o da etapa de segmenta\xe7\xe3o da imagem, e posteriormente \xe9 apresentado uma estrat\xe9gia para tratar situa\xe7\xf5es com dois objetos da mesma cor. Por fim \xe9 demonstrado o uso do Filtro de Kalmam."),i.a.createElement("p",null,"\xdaltima atualiza\xe7\xe3o:  13 de Dezembro de 2015."),i.a.createElement("h3",null,"INTRODU\xc7\xc3O"),i.a.createElement("p",null,"O rastreamento de objetos \xe9 uma das mais importantes \xe1reas da vis\xe3o computacional, com extensas aplica\xe7\xf5es tanto para ind\xfastria pesada como automobilismo, assim como para a ind\xfastria do entretenimento, al\xe9m de ser uma poderosa ferramenta na \xe1rea m\xe9dica (Pinho et al., 2004). Vis\xe3o computacional consiste em t\xe9cnicas computacionais no qual possibilita interpretar imagens (WANGENHEIM et al., 2001). Segundo (Freitas et al.,2010), as principais aplica\xe7\xf5es do rastreamento de objetos em imagens s\xe3o para diagn\xf3sticos m\xe9dicos, interfaces Homem-Computador para controle de jogos eletr\xf4nicos e na \xe1rea de seguran\xe7a, para o monitoramento de ambientes com grandes fluxos de pessoas, tais como aeroportos, plataformas de trens e estacionamentos. O objetivo principal na \xe1rea de seguran\xe7a \xe9 detectar atrav\xe9s dos sistemas de rastreamento de objetos atividades indesejadas, contribuindo para a tomada de decis\xf5es dos profissionais de seguran\xe7a (Relli, 2014).Um algoritmo de rastreamento de objetos busca a partir de cenas provindas de um sensor \xf3ptico, como uma c\xe2mera, identificar a trajet\xf3ria que um ou mais objetos descrevem. No entanto, existem diversos fatores que dificultam a identifica\xe7\xe3o da trajet\xf3ria descrita por um objeto no mundo real. Seja por varia\xe7\xf5es de ilumina\xe7\xe3o, como o ascender ou apagar de luzes, ru\xeddos de fundo e principalmente oclus\xf5es que eventualmente o objeto sofra (Weng et al., 2006). Para contornar as dificuldades do mundo real para o rastreamento de objetos, \xe9 feito o uso de diversas estrat\xe9gias de predi\xe7\xe3o como o Filtro de Part\xedculas e o Filtro de Kalman (Iraei and Faez, 2015).Os sistemas de rastreamento de objetos usando vis\xe3o computacional podem ser divido em tr\xeas est\xe1gios, conforme ilustrado na Figura 1. O primeiro est\xe1gio \xe9 onde ocorre a segmenta\xe7\xe3o da imagem, o segundo est\xe1gio \xe9 onde faz-se o rastreamento ao longo do tempo do objeto ou alvo (",i.a.createElement("i",null,"target"),") e no ultimo est\xe1gio, faz-se a classifica\xe7\xe3o dos objetos quanto a suas a\xe7\xf5es executadas."),i.a.createElement(E,{src:"https://3.bp.blogspot.com/-Mad91z12UTo/WV7e5j1RkRI/AAAAAAAAAV8/1uTcrkXLTxodkvw2r_DDTnEBA3smrcfAQCLcBGAs/s320/sys_rast.png",alt:"Workflow",legend:"Figura 1 - Etapas"}),i.a.createElement("h3",null,"O Filtro de Kalman "),i.a.createElement("p",null,"O Filtro de Kalman consiste em um conjunto de equa\xe7\xf5es que possibilitam a implementa\xe7\xe3o recursiva de um estimador, gerando predi\xe7\xe3o \xf3tima dos estados futuros de um sistema linear a partir de uma observa\xe7\xe3o presente (Welch and Bishop, 1995). Foi desenvolvido em meados de 1960 por Rudolf Emil Kalman (Kalman et al., 1960), inicialmente para aplica\xe7\xf5es aeroespaciais. No entanto, logo vislumbraram-se diversas aplica\xe7\xf5es em outras \xe1reas, como rob\xf3tica m\xf3vel, rastreamento de alvos, identifica\xe7\xe3o de sistemas, controle de processos, an\xe1lise e processamento de sinais entre outros (Funk, 2003). Existem hoje varia\xe7\xf5es para sistemas n\xe3o-lineares, como o Filtro de Kalman Estendido (EKF) e o Filtro de Kalman Unscented (UKF). Neste trabalho ser\xe1 feito o uso do Filtro de Kalman linear (KF) que busca gerar estimativas \xf3timas dos estados de um sistema descrito por"),i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r x_{k+1}&=Ax_k+Bu_k+w_k\\\\\r y_k &= Cx_k + v_k\\end{aligned}"}),"No qual ",i.a.createElement(b,{math:"x_k \\in \\mathbb{R}^n"})," \xe9 o vetor de estados, ",i.a.createElement(b,{math:"A \\in \\mathbb{R}^{n\\times n}"})," \xe9 a matriz de estado, ",i.a.createElement(b,{math:"B \\in \\mathbb{R}^{n\\times m}"})," \xe9 a matriz de entrada, ",i.a.createElement(b,{math:"u_k \\in \\mathbb{R}^m"})," \xe9 o vetor de entrada, ",i.a.createElement(b,{math:"w_k"})," representa a incerteza associada a modelagem do processo, no qual \xe9 assumido como sendo uma distribui\xe7\xe3o gaussiana, com m\xe9dia nula, ",i.a.createElement(b,{math:"y_k \\in \\mathbb{R}^p"})," o vetor de sa\xedda, ",i.a.createElement(b,{math:"C \\in \\mathbb{R}^{p\\times n}"})," a matriz de sa\xedda e ",i.a.createElement(b,{math:"v_k"})," a incerteza associada a medi\xe7\xe3o da sa\xedda. Da mesma forma, $v_k$ \xe9 assumido como sendo gaussiano, com m\xe9dia nula e ",i.a.createElement(b,{math:"w_k"})," e ",i.a.createElement(b,{math:"v_k"})," n\xe3o possuem correla\xe7\xe3o. Para este caso, o filtro de Kalman pode ser implementado por:",i.a.createElement(p.BlockMath,{math:"\r \\begin{aligned}\r \\hat{x}_{k+1|k} &= A\\hat{x}_{k|k}+Bu_{k}\\\\\r P_{k+1|k}&=AP_{k|k} A^T+Q\\\\\r K_k&=P_{k+1|k} C^T(R +CP_{k+1|k} C^T)^{-1} \\\\\r \\hat{x}_{k+1|k+1} &=\\hat{x}_{k+1|k}+K_k(y_{k+1}-C\\hat{x}_{k+1|k}) \\\\\r P_{k+1|k+1}&= (I-K_kC)P_{k+1|k}(I-K_kC)^T+K_kRK_k^T \r \\end{aligned}"}),i.a.createElement("p",null,"Com ",i.a.createElement(b,{math:"P \\in \\mathbb{R}^{n\\times n}"})," sendo a matriz de covari\xe2ncia da estimativa, ",i.a.createElement(b,{math:"K \\in \\mathbb{R}^{m\\times n}"})," o ganho \xf3timo de Kalman, ",i.a.createElement(b,{math:"Q \\in \\mathbb{R}^{n\\times n}"})," a matriz de covari\xe2ncia do modelo, ",i.a.createElement(b,{math:"R \\in \\mathbb{R}^{m\\times m}"})," a matriz de covari\xe2ncia das entradas, ",i.a.createElement(b,{math:"I \\in  \\mathbb{R}^{n \\times n}"})," \xe9 a matriz identidade de dimens\xe3o compt\xedvel e ",i.a.createElement(b,{math:"\\hat{x}_k"})," \xe9 o vetor de estimativas dos estados no instante ",i.a.createElement(b,{math:"k"}),". O Filtro de Kalman funciona em duas etapas, chamadas de predi\xe7\xe3o e corre\xe7\xe3o. Na etapa de predi\xe7\xe3o o filtro gera uma estimativa ",i.a.createElement("i",null,"a priori")," do vetor de estados, e na etapa de corre\xe7\xe3o, caso dispon\xedvel, o filtro toma uma medi\xe7\xe3o provinda de um sensor e faz a atualiza\xe7\xe3o, gerando uma estimativa ",i.a.createElement("i",null,"a posteriori"),". Note que nas equa\xe7\xf5es do filtro, a nota\xe7\xe3o k+1|k indica o instante $k+1$ ",i.a.createElement("i",null,"a priori"),", ou seja, n\xe3o possuindo ainda uma medi\xe7\xe3o, enquanto a nota\xe7\xe3o k+1|k+1 indica o instante k+1 dado que j\xe1 \xe9 conhecido uma medi\xe7\xe3o. O ciclo de funcionamento do filtro \xe9 ilustrado na Figura 2."),i.a.createElement(E,{src:"https://2.bp.blogspot.com/-gF3dW3XMZLE/WV7fa-0sBLI/AAAAAAAAAWA/y8jpvSb5ngYka6SS4QW00z_dlTPvBvAXQCLcBGAs/s320/predicao.png",alt:"Etapas do Filtro de Kalman",legend:"Figura 2 - Filtro de Kalman"}),i.a.createElement("p",null,"As matrizes ",i.a.createElement(b,{math:"Q"})," e ",i.a.createElement(b,{math:"R"}),' s\xe3o par\xe2metros de sintonia do filtro de Kalman, no qual possibilitam fazer com que ele passe a "confiar" mais na modelagem, conforme ilustrado na Figura 3, ou na medi\xe7\xe3o, conforme ilustrado na Figura 4. As respectivas figuras apresentam as Fun\xe7\xf5es de Densidade de Probabilidade (FDP) da sa\xedda do modelo, das medi\xe7\xf5es e da sa\xedda do Filtro de Kalman. Para ilustrar o comportamento do filtro, suponha a matriz ',i.a.createElement(b,{math:"Q=qI"})," e a matriz ",i.a.createElement(b,{math:"R=rI"}),", sendo ",i.a.createElement(b,{math:"I"})," a matriz identidade de dimens\xe3o comp\xe1tivel e ",i.a.createElement(b,{math:"q,r \\in \\mathbb{R}"}),". Note que para o caso no qual ",i.a.createElement(b,{math:"q<r"}),", a FDP do Filtro de Kalman est\xe1 mais pr\xf3xima da FDP do modelo. Ou seja, nesse caso, o filtro est\xe1 tendendo a gerar sa\xeddas pr\xf3ximas as do modelo. E no caso que ",i.a.createElement(b,{math:"q>r"}),", o filtro apresenta uma FDP mais pr\xf3xima da FDP das medidas. Assim a sa\xedda do filtro tende a gerar valores pr\xf3ximos aos medidos."),i.a.createElement(E,{src:"https://2.bp.blogspot.com/--KLJOSzaIhI/WV7gPsqjMKI/AAAAAAAAAWE/UjdyAw1ToOgbsD2osrlHaYFLU1maoV0YwCLcBGAs/s320/kalman_q.jpg",alt:"Confian\xe7a do filtro",legend:"Figura 3 - Maior confian\xe7a no modelo."}),i.a.createElement(E,{src:"https://2.bp.blogspot.com/-xBBvXPJZYPs/WV7gPuUxFqI/AAAAAAAAAWI/id0-JAHpvuIofQ2b3s527-EMv4vBoFFIQCLcBGAs/s320/kalman_r.jpg",alt:"Confian\xe7a do filtro",legend:"Figura 4 - Maior confian\xe7a na medi\xe7\xe3o."}),i.a.createElement("p",null,"O objetivo principal deste trabalho \xe9 apresentar de forma did\xe1tica as principais etapas de implementa\xe7\xe3o de um sistema de rastreamento de objetos em tempo real. Fazendo uso de abordagens encontradas na literatura. Para aquisi\xe7\xe3o da imagem, \xe9 utilizado uma c\xe2mera de baixo custo (",i.a.createElement("i",null,"webcam"),") e a plataforma de programa\xe7\xe3o Matlab\xae, no qual j\xe1 conta com diversas ferramentas para processamento de imagens. O sistema de rastreamento apresentado visa rastrear o maior objeto na cor vermelha presente na cena. E ainda lidar tamb\xe9m com situa\xe7\xf5es de r\xe1pidas oclus\xf5es, parciais ou totais, atrav\xe9s do uso do Filtro de Kalman."),i.a.createElement("h3",null,"SEGMENTA\xc7\xc3O DA IMAGEM"),i.a.createElement("p",null,"Conforme mencionado anteriormente para todo algoritmo de rastreamento de objetos em vis\xe3o computacional, existe um est\xe1gio de segmenta\xe7\xe3o, de forma a identificar em cada quadro, provindo da c\xe2mera, a posi\xe7\xe3o do objeto. Uma das estrat\xe9gias mais simples para a identifica\xe7\xe3o de objetos numa cena \xe9 atrav\xe9s de um processo de limiariza\xe7\xe3o. A limiariza\xe7\xe3o \xe9 uma das abordagens mais importantes da segmenta\xe7\xe3o de imagens. O princ\xedpio da limiariza\xe7\xe3o consiste em separar as regi\xf5es da imagem em duas classes, o fundo (",i.a.createElement("i",null,"background"),") e o objeto (",i.a.createElement("i",null,"target"),") (ARTERO and TOMMASELLI, 2000).Neste trabalho foi optado por trabalhar com imagens no espa\xe7o RGB (",i.a.createElement("i",null,"Red, Green e Blue"),"). Por ser este trabalho voltado para aplica\xe7\xf5es em tempo real, o espa\xe7o de cores RGB demonstra-se computacionalmente menos custoso, pois em geral, os dispositivos de aquisi\xe7\xe3o de imagens j\xe1 trabalham neste padr\xe3o, n\xe3o sendo necess\xe1rio uma etapa de transforma\xe7\xe3o de espa\xe7o de cores. Assim as imagens obtidas pelo dispositivo de captura s\xe3o em geral formadas por tr\xeas canais de cores, representadas por matrizes. Sendo que as entradas das matrizes s\xe3o respectivamente a informa\xe7\xe3o relativa ao vermelho, verde e azul para cada ",i.a.createElement("i",null,"pixel"),", conforme ilustrado na Figura 5."),i.a.createElement(E,{src:"https://2.bp.blogspot.com/--w3ugv-udTs/WV7hd-7ZFNI/AAAAAAAAAWQ/M4cYdEph-yIwHy2ko6LT4kmAJkOFx7nUACLcBGAs/s320/rgb.png",alt:"imagem rgb",legend:"Figura 5 - Imagem RGB."}),i.a.createElement("p",null,"A estrat\xe9gia de limiariza\xe7\xe3o adotada neste trabalho foi a subtra\xe7\xe3o dos canais de cores verde e azul do canal de cor vermelho, uma vez que busca-se rastrear os objetos na cor vermelha presente na cena. E ent\xe3o considerou-se um valor limiar (",i.a.createElement("i",null,"threshold"),"), de forma que os ",i.a.createElement("i",null,"pixels")," resultantes com valores inferiores a este limiar s\xe3o descartados e os ",i.a.createElement("i",null,"pixels")," com valores maiores s\xe3o considerados como parte do objeto a ser rastreado, conforme apresentado na Figura 6."),i.a.createElement(E,{src:"https://4.bp.blogspot.com/-pnYFUlaJSms/WV7ibAa9MBI/AAAAAAAAAWY/nizRhmsKjTUn6y0rjASWBKx82LuQLNbfACLcBGAs/s320/limiar.png",alt:"Limiariza\xe7\xe3o",legend:"Figura 6 - Limiariza\xe7\xe3o."}),i.a.createElement("i",null,"Obs: O valor de ",i.a.createElement(b,{math:"L"})," foi obtido empiricamente, atrav\xe9s de v\xe1rios testes. At\xe9 chegar no valor ideal para as condi\xe7\xf5es de ilumina\xe7\xe3o no qual a c\xe2mera se encontrava no momento da implementa\xe7\xe3o."),i.a.createElement("p",null,"Como resultado da limiariza\xe7\xe3o \xe9 obtido uma imagem bin\xe1ria, ou seja, cujo os ",i.a.createElement("i",null,"pixels")," possuem valores de 0 ou 1, resultando em uma imagem do tipo preto e branca, no qual a regi\xe3o branca representa o objeto vermelho presente na cena. Na Figura 7 \xe9 apresentado o resultado obtido."),i.a.createElement(E,{src:"https://3.bp.blogspot.com/-hO9_WqmaZUg/WV7itsUjNGI/AAAAAAAAAWc/34FZnxTwtXEZ4eJnJUeWSkv4daxUOPEeACLcBGAs/s320/bin.png",alt:"Resultado",legend:"Figura 7 - Resultado da Limiariza\xe7\xe3o."}),i.a.createElement("h4",null,"Tratando dois objetos vermelhos na cena"),i.a.createElement("p",null,"Uma situa\xe7\xe3o poss\xedvel no qual \xe9 desejado que o algoritmo apresente robustez, \xe9 no caso de existirem dois objetos na cor vermelha presente na imagem, ou mesmo a presen\xe7a de pequenos detalhes vermelhos no fundo da imagem. O resultado da limiariza\xe7\xe3o para este caso, possui duas ou mais regi\xf5es brancas conforme a Figura 8, no qual apresenta o resultado da limiariza\xe7\xe3o quando \xe9 posicionado dois objetos vermelhos diante da c\xe2mera."),i.a.createElement(E,{src:"https://1.bp.blogspot.com/-HugKp3SFnYo/WV7jDSoUkTI/AAAAAAAAAWg/NGsh3PWAebo0XRJC5SFmx6LYn27jLCdpgCLcBGAs/s320/bin_multo.png",alt:"Limiariza\xe7\xe3o com dois objetos",legend:"Figura 8 - Limiariza\xe7\xe3o com dois objetos."}),i.a.createElement("p",null,"O objetivo deste trabalho \xe9 rastrear o maior objeto vermelho presente na cena. Portanto, faz-se necess\xe1rio a implementa\xe7\xe3o de um mecanismo para buscar a posi\xe7\xe3o do maior objeto vermelho. Visando o m\xednimo de consumo computacional, de forma a garantir um bom funcionamento em tempo real, foi implementado o algoritmo que faz a acumula\xe7\xe3o dos ",i.a.createElement("i",null,"pixels")," da imagem bin\xe1ria, tanto na horizontal, como na vertical. Define-se a imagem bin\xe1ria como sendo uma matriz ",i.a.createElement(b,{math:"O \\in \\mathbb{N}^{N\\times M}"})," definida como ",i.a.createElement(b,{math:"O = \\{o_{ij}\\}"}),", com ",i.a.createElement(b,{math:"1\\leq i \\leq N"})," e ",i.a.createElement(b,{math:"1\\leq j\r \\leq M"}),", cuja entradas s\xe3o 0 ou 1. O vetor de acumula\xe7\xe3o horizontal ",i.a.createElement(b,{math:"H \\in \\mathbb{N}^N"})," \xe9 definido como:",i.a.createElement(p.BlockMath,{math:"\r \\begin{aligned}H=[h_1~h_2~\\cdots~ h_N]^T ,\\quad h_i=\\sum_{j=1}^{M}o_{ij}, \\quad\r i=1,2,\\ldots,N\\end{aligned}"}),"E o vetor de acumula\xe7\xe3o vertical ",i.a.createElement(b,{math:"V \\in \\mathbb{N}^M"}),"como:"),i.a.createElement(p.BlockMath,{math:"\r \\begin{aligned}V=[v_1~v_2~\\cdots~ v_M]^T ,\\quad v_j=\\sum_{i=1}^{N}o_{ij}, \\quad\r j=1,2,\\ldots,M\\end{aligned}"}),i.a.createElement("p",null,"Dessa forma, o ponto ",i.a.createElement(b,{math:"(x_{max},y_{max})"})," com a maior concentra\xe7\xe3o de pixels vermelhos na imagem \xe9 dado por:"),i.a.createElement(p.BlockMath,{math:"\r \\begin{aligned}x_{max}=max(V), \\quad\r y_{max}=max(H)\\end{aligned}"}),i.a.createElement("p",null,"Na Figura 9 \xe9 ilustrado a acumula\xe7\xe3o dos ",i.a.createElement("i",null,"pixels")," da imagem bin\xe1ria e os respectivos pontos de m\xe1ximo, que coincidem com o ponto na imagem que cont\xe9m a maior concentra\xe7\xe3o de",i.a.createElement("i",null,"pixels")," vermelhos."),i.a.createElement(E,{src:"https://1.bp.blogspot.com/-btt2yn24EY4/WV7jXqyCaPI/AAAAAAAAAWk/o_w3fjIhM2wIqeWjF-LIFgLZtPm12ULswCLcBGAs/s320/acumulacao.png",alt:"Acumula\xe7\xe3o",legend:"Figura 9 - Acumula\xe7\xe3o"}),i.a.createElement("p",null," Ap\xf3s a identifica\xe7\xe3o da regi\xe3o onde possui o maior objeto vermelho na cena, define-se uma regi\xe3o de interesse, formada considerando-se o intervalo de $10\\%$, para cima, para baixo e para os lados, em torno do ponto",i.a.createElement(b,{math:"(x_{max},y_{max})"}),". Em seguida \xe9 determinado a coordenada ",i.a.createElement(b,{math:"(x_{c},y_{c})"})," do objeto aplicando-se o c\xe1lculo do centro geom\xe9trico na regi\xe3o de interesse, atrav\xe9s das equa\xe7\xf5es:"),i.a.createElement(p.BlockMath,{math:"      \r \\begin{aligned}\r x_c &=\\frac{\\sum_{i=1}^{N}o_{ij}i}{\\sum_{i=1}^{N}\\sum_{j=1}^{M}o_{ij}}\\\\\r y_c &=\\frac{\\sum_{i=1}^{M}o_{ij}j}{\\sum_{i=1}^{N}\\sum_{j=1}^{M}o_{ij}}\r \\end{aligned}"}),i.a.createElement("h3",null,"TRATAMENTO DE OCLUS\xd5ES"),i.a.createElement("p",null,"Note que para o caso no qual \xe9 poss\xedvel visualizar o objeto na cena, o procedimento apresentado na se\xe7\xe3o anterior \xe9 suficiente para fazer o rastreamento. Por\xe9m, caso este objeto sofra uma oclus\xe3o, o procedimento descrito falha em buscar as coordenadas do objeto. Para contornar este problema, foi tomado como ferramenta o Filtro de Kalman e ent\xe3o, nas situa\xe7\xf5es de oclus\xe3o, n\xe3o mais \xe9 feito o processamento da imagem, mas \xe9 gerado estimativas da posi\xe7\xe3o do objeto baseando-se no ultimo instante no qual foi poss\xedvel visualizar o objeto.Para implementar o Filtro de Kalman deve-se considerar um modelo para a din\xe2mica do movimento do objeto, neste trabalho optou-se por utilizar o modelo linear dado por:"),i.a.createElement(p.BlockMath,{math:"\r \\begin{aligned}\r \\left[\r \\begin{array}{cccc}\r \\hat{x}_1(k+1) \\\\ \r \\hat{x}_2(k+1) \\\\ \r \\hat{x}_3(k+1) \\\\ \r \\hat{x}_4(k+1)\r \\end{array} \\right] =\\left[ \\begin{array}{cccc}\r 1 & 0 & \\Delta t & 0 \\\\\r 0 & 1 & 0 & \\Delta t \\\\\r 0 & 0 & 1 & 0\\\\\r 0 & 0 & 0 & 1\r \\end{array} \\right] \\left[ \\begin{array}{cccc}\r \\hat{x}_1(k) \\\\\r \\hat{x}_2(k) \\\\\r \\hat{x}_3(k) \\\\\r \\hat{x}_4(k)\\end{array} \\right]+\\left[ \\begin{array}{cccc}\r \\frac{1}{2}\\Delta t^2 & 0 \\\\\r 0 & \\frac{1}{2}\\Delta t^2\\\\\r \\Delta t & 0 \\\\\r 0 & \\Delta t \\\\\r \\end{array} \\right]\\left[\r \\begin{array}{cccc}\r u_1(k) \\\\\r u_2(k) \\end{array} \\right],\\\\\r \\left[\r \\begin{array}{cccc}\r z_1(k) \\\\\r z_2(k)\\end{array} \\right]=\\left[\r \\begin{array}{cccc}\r 1 & 0 & 0 & 0 \\\\\r 0 & 1 & 0 & 0\\end{array} \\right]\\left[ \\begin{array}{cccc}\r \\hat{x}_1(k) \\\\\r \\hat{x}_2(k) \\\\\r \\hat{x}_3(k) \\\\\r \\hat{x}_4(k) \\end{array}\r \\right]\\end{aligned}"}),i.a.createElement("p",null,"Sendo que ",i.a.createElement(b,{math:"\\hat{x}_1=\\hat{x}_c"})," \xe9 a estimativa da coordenada horizontal, ",i.a.createElement(b,{math:"\\hat{x}_2=\\hat{y}_c"})," \xe9 a estimativa da coordenada vertical, ",i.a.createElement(b,{math:"\\hat{x}_3=\\hat{v}_x"})," \xe9 a estimativa da velocidade na horizontal, ",i.a.createElement(b,{math:"\\hat{x}_4=\\hat{v}_y"})," \xe9 a estimativa da velocidade na vertical, ",i.a.createElement(b,{math:"u_1=a_x"})," \xe9 a acelera\xe7\xe3o horizontal e ",i.a.createElement(b,{math:"u_2=a_y"})," \xe9 a acelera\xe7\xe3o vertical. Note que o modelo descreve um movimento retil\xedneo uniformemente variado (MURV) e o $\\Delta t$ presente, indica o tempo de amostragem, que para este caso, \xe9 o tempo no qual o Matlab\xae leva para processar cada quadro da cena."),i.a.createElement("i",null,"Obs: Os valores de ",i.a.createElement(b,{math:"a_x"})," e ",i.a.createElement(b,{math:"a_y"})," s\xe3o obtidos atrav\xe9s dos frames anteriores, fazendo aproxima\xe7\xe3o da derivada segunda da posi\xe7\xe3o."),i.a.createElement("p",null,'Com o modelo definido, pode-se aplicar as equa\xe7\xf5es do Filtro de Kalman apresentadas na introdu\xe7\xe3o e ent\xe3o gerar estimativas para a posi\xe7\xe3o do objeto vermelho. Por\xe9m, como \xe9 desejado o tratamento de oclus\xf5es, foi optado por utilizar n\xe3o apenas um, mas dois Filtros de Kalman, sendo que o primeiro \xe9 sintonizado para ter "confian\xe7a" na medi\xe7\xe3o. E o segundo "confian\xe7a" no modelo. Assim obt\xeam-se um algoritmo com maior robustez. A Figura 10 apresenta o diagrama conceitual da estrutura utilizada.'),i.a.createElement(E,{src:"https://1.bp.blogspot.com/-XpoyRu0bpdc/WV7jsc2OIlI/AAAAAAAAAWo/pZh80J0V2LQqaG3vI_MHqad1PTcIiyIAwCLcBGAs/s320/diagram.png",alt:"estrutura",legend:"Figura 10 - Estrutura."}),i.a.createElement("p",null,"Note que a sa\xedda passa a ser ",i.a.createElement(b,{math:"Y_1"})," e ",i.a.createElement(b,{math:"Y_2"}),', que s\xe3o selecionadas conforme a detec\xe7\xe3o ou n\xe3o de oclus\xf5es. Nos instantes em que n\xe3o existe oclus\xe3o, a sa\xedda \xe9 aquela provinda do Filtro de Kalman que "confia" mais na medi\xe7\xe3o. E quando verifica-se uma oclus\xe3o, \xe9 selecionado a sa\xedda do Filtro de Kalman que "confia" mais no modelo. Esta estrat\xe9gia foi necess\xe1ria pois, o filtro sintonizado para confiar no modelo apresenta bons resultados nas situa\xe7\xf5es de oclus\xe3o, por\xe9m uma baixa efici\xeancia na situa\xe7\xf5es sem oclus\xe3o e vice-versa.'),i.a.createElement("h3",null,"RESULTADO"),"O resultado obtido pelo algoritmo de rastreamento desenvolvido \xe9 apresentado nesta se\xe7\xe3o. Para situa\xe7\xf5es sem oclus\xe3o, o resultado \xe9 conforme apresentado na Figura 11.",i.a.createElement(E,{src:"https://4.bp.blogspot.com/-yAXdYw8HpeI/WV7j6jeVzHI/AAAAAAAAAWs/cgOV-Jz_lqYWKwdVxT8POMYlABE7guPJQCLcBGAs/s320/frame_so.png",alt:"sem oclus\xe3o",legend:"Figura 11 - Sem oclus\xe3o."}),"Para as situa\xe7\xf5es com oclus\xe3o, o resultado \xe9 apresentado na Figura 12.",i.a.createElement(E,{src:"https://1.bp.blogspot.com/-DESUcQuAcL0/WV7kHCAeJzI/AAAAAAAAAWw/QNZ-F05mHA0QI_gjyHi_Ao0_g0v-Ti5qACLcBGAs/s320/frame_co.png",alt:"Com oclus\xe3o",legend:"Figura 12 - Com oclus\xe3o"}),'Nos instantes em que n\xe3o \xe9 poss\xedvel visualizar o objeto, \xe9 tomado os valores obtidos pelo Filtro de Kalman sintonizado para "confiar" no modelo, ent\xe3o baseando-se no ultimo instante que foi poss\xedvel visualizar o objeto, \xe9 gerado estimativas da trajet\xf3ria do objeto conforme apresentado na Figura 13.',i.a.createElement(E,{src:"https://2.bp.blogspot.com/-HTJYIdaKPfo/WV7ksya140I/AAAAAAAAAW0/mVVB5ByEcfEHSD14V-LR56SWBKW8Moj7QCLcBGAs/s320/trajetoria.jpg",alt:"trajet\xf3ria",legend:"Figura 13 - Trajet\xf3ria."}),'Note que como o modelo utilizado \xe9 linear, a estimativa obtida \xe9 de uma trajet\xf3ria retil\xednea. No entanto, para situa\xe7\xf5es sem oclus\xe3o, por ser utilizado um filtro sintonizado para "confiar" na medi\xe7\xe3o, obt\xeam-se um bom desempenho para movimentos n\xe3o-lineares, conforme apresentado na Figura 14.',i.a.createElement(E,{src:"https://2.bp.blogspot.com/-WSzLStoHZcE/WV7lE7OI4LI/AAAAAAAAAW4/x2HsJzoqSawuF4gBJ_cjsNQQQjFWFFFkACLcBGAs/s320/trajet2.jpg",alt:"n\xe3o-linear",legend:"Figura 14 - Movimentos n\xe3o-lineares"}),i.a.createElement("h3",null,"V\xcdDEO DEMONSTRA\xc7\xc3O"),i.a.createElement("center",null,i.a.createElement("iframe",{allowfullscreen:"",class:"YOUTUBE-iframe-video","data-thumbnail-src":"https://i.ytimg.com/vi/tREbIw9DxHA/0.jpg",frameborder:"0",height:"480",src:"https://www.youtube.com/embed/tREbIw9DxHA?feature=player_embedded",width:"650"})),i.a.createElement("h3",null,"C\xd3DIGO FONTE"),i.a.createElement("center",null,i.a.createElement("a",{href:"https://github.com/Marofe/Object-Tracking/blob/master/kalman_live.m"},"https://github.com/Marofe/Object-Tracking/blob/master/kalman_live.m")),i.a.createElement("h3",null,"CONCLUS\xc3O"),i.a.createElement("p",null,"Com o desenvolvimento deste trabalho foi poss\xedvel verificar na pr\xe1tica o desempenho do Filtro de Kalman para estimar a trajet\xf3ria de objetos com situa\xe7\xf5es no qual existe falta de informa\xe7\xe3o. Tamb\xe9m foi apresentado os principais detalhes de implementa\xe7\xe3o do sistema de vis\xe3o computacional, voltando-se para a \xe1rea de rastreamento de objetos. Mostrou-se que \xe9 poss\xedvel obter um desempenho satisfat\xf3rio para rastreamento de objetos em cenas obtidas por uma c\xe2mera de baixo custo, mesmo com a presen\xe7a de mais de um objeto da mesma cor. O algoritmo apresentou boa efici\xeancia para situa\xe7\xf5es de r\xe1pidas oclus\xf5es observou-se que este projeto ilustra de forma simples o potencial do Filtro de Kalman e sua relativa simplicidade de implementa\xe7\xe3o."),i.a.createElement("h3",null,"REFER\xcaNCIAS"),i.a.createElement("p",null,i.a.createElement("b",null,"ARTERO, A. and TOMMASELLI, A. (2000)"),". Limiariza\xe7\xe3o autom\xe1tica de imagens digitais, Boletim de Ci\xeancias Geod\xe9sicas 6(1): 38\u201348."),i.a.createElement("p",null,i.a.createElement("b",null,"Freitas, G. M. et al. (2010)"),". Rastreamento de objetos em v\xeddeos e separa\xe7\xe3o em classes."),i.a.createElement("p",null,i.a.createElement("b",null,"Funk, N. (2003)"),". A study of the kalman filter applied to visual tracking, University of Alberta, Project for CMPUT 652(6)."),i.a.createElement("p",null,i.a.createElement("b",null,"Iraei, I. and Faez, K. (2015)"),". Object tracking with occlusion handling using mean shift, kalman filter and edge histogram, Pattern Recognition and Image Analysis (IPRIA), 2015 2nd International Conference on, IEEE, pp. 1\u20136."),i.a.createElement("p",null,i.a.createElement("b",null,"Kalman, R. E. et al. (1960)"),". A new approach to linear filtering and prediction problems, Journal of basic Engineering 82(1): 35\u201345."),i.a.createElement("p",null,i.a.createElement("b",null,"Pinho, R. R., Tavares, J. M. R. S. and Correia, M. F. P. V. (2004).")," Introdu\xe7\xe3o \xe0 an\xe1lise de movimento usando vis\xe3o computacional."),i.a.createElement("p",null,i.a.createElement("b",null,"Relli, C. (2014)"),". Caracteriza\xe7\xe3o de algoritmos de rastreamento de objetos em video considerando situa\xe7\xf5es de oclus\xe3o, RETEC-Revista de Tecnologias 6(1)."),i.a.createElement("p",null,i.a.createElement("b",null,"Van den Bergh, M. and Van Gool, L. (2011)"),". Combining rgb and tof cameras for real-time 3d hand gesture interaction, Applications of Computer Vision (WACV), 2011 IEEE Workshop on, IEEE,p. 66\u201372."),i.a.createElement("p",null,i.a.createElement("b",null,"WANGENHEIM, A. v. et al. (2001)"),". Seminario introdu\xe7\xe3o a vis\xe3o computacional, Vis\xe3o Computacional Aldon von Wangenheim\u2019s HomePage."),i.a.createElement("p",null,i.a.createElement("b",null,"Welch, G. and Bishop, G. (1995)"),". An introduction to the kalman filter."),i.a.createElement("p",null,i.a.createElement("b",null,"Weng, S.-K., Kuo, C.-M. and Tu, S.-K. (2006)"),". Video object tracking using adaptive kalman filter,Journal of Visual Communication and Image Representation 17(6): 1190\u20131208."),i.a.createElement(k.a.DiscussionEmbed,{shortname:"marofe-github-io",config:{url:"https://marofe.github.io/?p=tutorials/rastreamento_usando_visao_filtro_kalman",identifier:"rastreamento_usando_visao_filtro_kalman",title:"Rastreamento de Objetos usando Vis\xe3o Computacional e Filtro de Kalman"}}))},y=function(e){return i.a.createElement("div",{className:"divPage"},i.a.createElement("div",{class:"listCell"},i.a.createElement("h2",null,i.a.createElement("a",{href:"/tutorials/rastreamento_usando_visao_filtro_kalman"},"Rastreamento de objetos usando Vis\xe3o Computacional e Filtro de Kalman")),i.a.createElement("p",null,"Este trabalho apresenta um algoritmo em tempo real para rastreamento de objetos em vis\xe3o computacional, usando o Filtro de Kalman como mecanismo de predi\xe7\xe3o para situa\xe7\xf5es de oclus\xe3o e ou contamina\xe7\xe3o da cena por ru\xeddo. O principal objetivo deste trabalho \xe9 de apresentar de forma did\xe1tica o desenvolvimento de um algoritmo de rastreamento de objetos baseado em cor. O algoritmo apresentado faz o rastreamento do maior objeto sim\xe9trico de uma cor pr\xe9-definida presente na cena. \xc9 apresentado em detalhes a implementa\xe7\xe3o da etapa de segmenta\xe7\xe3o da imagem, e posteriormente \xe9 apresentado uma estrat\xe9gia para tratar situa\xe7\xf5es com dois objetos da mesma cor. Por fim \xe9 demonstrado o uso do Filtro de Kalmam."),i.a.createElement("p",null,"\xdaltima atualiza\xe7\xe3o:  13 de Dezembro de 2015.")))},v=function(){return i.a.createElement("div",{className:"divPage"},i.a.createElement(d.a,null,i.a.createElement("title",null,"Tutorials | Marcos Rog\xe9rio Fernandes"),i.a.createElement("meta",{name:"description",content:"Here you will find some of my tutorials and toy examples. "})),i.a.createElement("div",{className:"top"},i.a.createElement("h1",null,"Tutorials")),i.a.createElement(m.a,{path:"/tutorials",exact:!0,render:function(e){return i.a.createElement(y,null)}}),i.a.createElement(m.a,{path:"/tutorials/rastreamento_usando_visao_filtro_kalman",exact:!0,render:function(e){return i.a.createElement(x,null)}}))},w=t(27),A=t(28),M=t(30),T=t(29),F=t(31),B=(t(17),function(e){var a=e.notes.map(function(e){return i.a.createElement("div",{className:"listCell"},i.a.createElement("h2",null,i.a.createElement("a",{href:"/notes/"+e.link},e.title)),i.a.createElement("p",null,e.desc))});return i.a.createElement("div",null,i.a.createElement(d.a,null,i.a.createElement("title",null,"Notes | Marcos Rog\xe9rio Fernandes"),i.a.createElement("meta",{name:"description",content:"Here you will find my research notes."})),i.a.createElement("div",{className:"top"},i.a.createElement("h1",null,"My Notes"),i.a.createElement("p",null,"The following contents are some of my research notes about a few topics that I have been working on.")),a)}),q=function(e){var a={url:"https://marofe.github.io/?p="+e.note.link,identifier:"note-"+e.note.link,title:e.title};return i.a.createElement("article",null,i.a.createElement(d.a,null,i.a.createElement("title",null,e.note.title,"| Marofe"),i.a.createElement("meta",{name:"description",content:e.note.desc})),i.a.createElement("h1",null,e.title),i.a.createElement("p",null,e.desc),i.a.createElement("p",{align:"right"},"Last Update:  26 December, 2019."),i.a.createElement("div",null,"Consider a dynamic system in which its states are embedded on a Matrix Lie Group ",i.a.createElement(b,{math:"G"})," of dimension ",i.a.createElement(b,{math:"n"})," and measurements are available through a map: ",i.a.createElement(b,{math:"h:G\\rightarrow G'"}),", where ",i.a.createElement(b,{math:"G'"})," is a Matrix Lie Group of dimension ",i.a.createElement(b,{math:"m"}),", described by",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r {X}_{k+1}&= {X}_k \\exp_G([ \\Omega( {X}_k, {u}_k)+  w_k]_G^\\wedge),\\\\\r {Y}_k &=   h( {X}_k)  \\exp_{G'}([  \\nu_k]_{G'}^\\wedge)\r\n\\end{aligned}"}),"where ",i.a.createElement(b,{math:"{X}_k\\in M\\subseteq G"})," is the state and ",i.a.createElement(b,{math:"{Y}_k \\in M'\\subseteq G'"})," is the measurement. ",i.a.createElement(b,{math:"M,M'"})," are subgroups of ",i.a.createElement(b,{math:"G,G'"}),", respectively, such that the following bijection is well defined",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\ln_G( \\exp_G( {X}_k))&= {X}_k,\\\\\r \\ln_{G'}( \\exp_{G'}( {Y}_k))&= {Y}_k\r\n\\end{aligned}"}),"and ",i.a.createElement(b,{math:"w_k\\sim \\mathcal{N}(0,Q_k)"})," and ",i.a.createElement(b,{math:"\\nu_k\\sim \\mathcal{N}(0,R_k)"}),".",i.a.createElement("center",null,i.a.createElement("h2",null,"Filtering")),i.a.createElement("b",null,"Prediction:"),i.a.createElement(p.BlockMath,{math:"\r\n\\begin{aligned}\r {\\hat{X}}_{k+1|k}&= {\\hat{X}}_{k|k}\\exp_G([\\hat{\\Omega}_k]_G^\\wedge)\\\\\r\nP_{k+1|k}&=\\mathscr{F}_kP_{k|k}\\mathscr{F}_k^{\\intercal}+\\Phi(\\hat{\\Omega}_k)Q_k\\Phi(\\hat{\\Omega}_k)^\\intercal\r\n\\end{aligned}"}),"where",i.a.createElement(p.BlockMath,{math:"\r\n\\begin{aligned}\r\n\\hat{\\Omega}_k&=\\Omega_k( {\\hat{X}}_{k|k},u_k)\\\\\r\n\\mathscr{F}_k&=Ad_G(\\exp_G([-\\hat\\Omega_k]_G^\\wedge))+\\Phi(\\hat\\Omega_k)\\mathscr{C}_k\\\\\r\n\\mathscr{C}_k&=\\frac{\\partial}{\\partial \\epsilon}\\Omega_k( {\\hat{X}}_{k|k}\\exp_G([\\epsilon]_G^\\wedge),u_k)|_{\\epsilon=0}\\\\\r\n\\Phi(a)&=\\sum_{m=0}^\\infty \\frac{(-1)^m}{(m+1)!}ad_G(a)^m\r\n\\end{aligned}"}),i.a.createElement("b",null,"Update:"),i.a.createElement(p.BlockMath,{math:"\r\n\\begin{aligned}\r\nK&=P_{k+1|k}\\mathscr{H}^\\intercal(R+\\mathscr{H}_kP_{k+1|k}\\mathscr{H}_k^T)^{-1}\\\\\r\nv_k&=K\\ln_{G'}(h( {\\hat{X}}_{k+1|k})^{-1}y_{k+1})^\\vee_{G'}\\\\\r {\\hat{X}}_{k+1|k+1}&= {\\hat{X}}_{k+1|k} \\exp([v_k]_G^\\wedge)\\\\\r\nP_{k+1|k+1}&=\\Phi(v_k)(I-K\\mathscr{H}_k)P_{k+1|k}\\Phi(v_k)^\\intercal\r\n\\end{aligned}"}),"where",i.a.createElement(p.BlockMath,{math:"\r\n\\begin{aligned}\r\n\\mathscr{H}_k=\\frac{\\partial}{\\partial \\epsilon}\\left[ \\ln_{G'}(h( {\\hat{X}}_{k+1|k})^{-1}h( {\\hat{X}}_{k+1|k} \\exp([ \\epsilon]^\\wedge_{G})))\\right]^{\\vee}_{G'}\\big|_{ \\epsilon=0}\r\n\\end{aligned}"})),i.a.createElement(k.a.DiscussionEmbed,{shortname:"marofe-github-io",config:a}))},N=function(e){var a={url:"https://marofe.github.io/?p="+e.note.link,identifier:"note-"+e.note.link,title:e.title};return i.a.createElement("article",null,i.a.createElement(d.a,null,i.a.createElement("title",null,e.note.title," | Marofe"),i.a.createElement("meta",{name:"description",content:e.note.desc})),i.a.createElement("h1",null,e.title),i.a.createElement("p",null,e.desc),i.a.createElement("p",{align:"right"},"Last Update:  25 Fev, 2020."),i.a.createElement("div",null,i.a.createElement("h2",null,"The Differential Riccati Equation"),"The selection of optimal control law and the design of optimal filters require the solving of a Riccati Equation. So, Riccati type equations emerge naturally on control and filtering problems of  dynamic systems. To find solutions for Riccati Equations, it is convenient to make subdivisions based on the nature of the coefficient matrices and the time interval considered.",i.a.createElement("ul",null,i.a.createElement("li",null,"Time-varying coefficients; ",i.a.createElement(b,{math:"t_1<\\infty"})),i.a.createElement("li",null,"Time-varying coefficients; ",i.a.createElement(b,{math:"t_1\\rightarrow \\infty"})),i.a.createElement("li",null,"Constant coefficients; ",i.a.createElement(b,{math:"t_1<\\infty"})),i.a.createElement("li",null,"Constant coefficient; ",i.a.createElement(b,{math:"t_1\\rightarrow \\infty"}))),"In particular, we restrict attention to the Riccati equation occurring to a situation when a boundary condition will be given at ",i.a.createElement(b,{math:"t_1"})," with the solution desired for ",i.a.createElement(b,{math:"t\\le t_1"}),'.  Recall that the Riccati Equation in the context of the control problem runs "backward" in time.',i.a.createElement(E,{src:"/images/p_finite.svg",width:"50%"}),"The following results are based on the book ",i.a.createElement("b",null,i.a.createElement("a",{href:"#Anderson1971"},"[Anderson1971]")),". It shows that in all four cases, it is possible to replace the problem of solving the ",i.a.createElement("i",null,"nonlinear")," Riccati differential equation by the problem of solving a ",i.a.createElement("i",null,"linear")," differential equation and then computing a matrix inverse.",i.a.createElement("h3",null,"Time-Varying Coeff. and Finite Horizon"),"Consider the differential Riccati equation for ",i.a.createElement(b,{math:"t \\le t_1"})," of the form",i.a.createElement(p.BlockMath,{math:"\r\n\\begin{aligned}\r -\\dot{P}=P(t)F(t)+F(t)^\\intercal P(t)-P(t)G(t)R^{-1}(t)G(t)^\\intercal P(t)+Q(t)\r\n\\end{aligned}"}),"and the following assumptions:",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\nP(t_1)&=P_1=P_1^\\intercal \\ge 0,\\\\\r\nQ(t)&=Q(t)^\\intercal \\ge 0,\\\\\r\nR(t)&=R(t)^\\intercal >0.\r\n\\end{aligned}"}),"Associated with the ",i.a.createElement("i",null,"Nonlinear Differential Riccati Equation")," we define an augmented ",i.a.createElement("i",null,"linear")," differential equation",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\left[\r \\begin{matrix}\r \\dot{X}(t)\\\\\r \\dot{Y}(t)\r \\end{matrix}\r \\right]=\\left[\r \\begin{matrix}\r F(t) & -G(t)R^{-1}(t)G(t)^\\intercal \\\\\r -Q(t) & -F(t)^\\intercal\r \\end{matrix}\r \\right]\\left[\r \\begin{matrix}\r X(t)\\\\\r Y(t)\r \\end{matrix}\r \\right],\\quad \\left[\r \\begin{matrix}\r X(t_1)\\\\\r Y(t_1)\r \\end{matrix}\r \\right]=\\left[\r \\begin{matrix}\r I\\\\\r P_1\r \\end{matrix}\r \\right].\r\n\\end{aligned}"}),i.a.createElement("div",{className:"lemma"},i.a.createElement("b",null,"Lemma:"),i.a.createElement("br",null),"If the solution of the Riccati Equation exists on ",i.a.createElement(b,{math:"[t,t_1]"})," then the solution of the linear counter-part has the property that ",i.a.createElement(b,{math:"X^{-1}(t)"})," also exists and",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r P(t)=Y(t)X^{-1}(t).\r\n\\end{aligned}"})),i.a.createElement("div",{className:"proof"},i.a.createElement("b",null,"Proof:"),i.a.createElement("br",null),"By the ",i.a.createElement("i",null,"product rule")," and ",i.a.createElement("i",null,"inverse rule")," of matrix calculus, one has",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r\n\\frac{d}{dt}[Y(t)X^{-1}(t)]&=\\dot{Y}(t)X^{-1}(t)-Y(t)X^{-1}(t)\\dot{X}(t)X^{-1}(t).\r\n\\end{aligned}"}),"Notice that from the augmented ODE we have",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r \\dot{X}(t)&=F(t)X(t)-G(t)R^{-1}(t)G(t)^\\intercal Y(t),\\\\\r \\dot{Y}(t)&=-Q(t)X(t)-F(t)^\\intercal Y(t)\r\n\\end{aligned}"}),"Thus,",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r\n\\frac{d}{dt}[YX^{-1}]&=(-QX-F^\\intercal Y)X^{-1}-YX^{-1}(FX-GR^{-1}G^\\intercal Y)X^{-1}\\\\\r\n&=-QXX^{-1}-F^\\intercal YX^{-1}-YX^{-1}FXX^{-1}+YX^{-1}GR^{-1}G^\\intercal YX^{-1}\\\\\r\n&=-Q-F^\\intercal (YX^{-1})-(YX^{-1})F+(YX^{-1})GR^{-1}G^\\intercal(YX^{-1})\\\\\r\n&=-Q-F^\\intercal P-PF+PGR^{-1}G^\\intercal P\r\n\\end{aligned}"}),"where for notation simplicity we adopt ",i.a.createElement(b,{math:"X(t)=X,Y(t)=Y,Q(t)=Q,F(t)=F,G(t)=G,R(t)=R"})," and ",i.a.createElement(b,{math:"P(t)=P"}),".  As result, we conclude that",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r\n-\\dot{P}=Q+F^\\intercal P+PF-PGR^{-1}G^\\intercal P.\r\n\\end{aligned}"}),"The preceding manipulations also shows that if ",i.a.createElement(b,{math:"X^{-1}(\\sigma)"})," exists for all ",i.a.createElement(b,{math:"\\sigma \\in [t,t_1]"})," then ",i.a.createElement(b,{math:"P(t)"})," exists for the same interval. Let us now check that the existence of ",i.a.createElement(b,{math:"P(t)"})," guarantees the existence of ",i.a.createElement(b,{math:"X^{-1}(t)"}),".",i.a.createElement("p",null),"Let ",i.a.createElement(b,{math:"\\Phi(\\cdot,\\cdot)"})," be the ",i.a.createElement("i",null,"Transition Matrix")," associated with the system",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\dot{X}=[F(t)-G(t)R^{-1}(t)G(t)^\\intercal P(t)]X(t).\r\n\\end{aligned}"}),"As long as ",i.a.createElement(b,{math:"P(t)"})," exists for ",i.a.createElement(b,{math:"t\\le t_1"})," then ",i.a.createElement(b,{math:"\\Phi(\\cdot,\\cdot)"})," is defined for all its arguments values less or equal to ",i.a.createElement(b,{math:"t_1"}),".  We claim that",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r X(t)=\\Phi(t,t_1),\\quad Y(t)=P(t)\\Phi(t,t_1).\r\n\\end{aligned}"}),"This is sufficient to prove that ",i.a.createElement(b,{math:"X^{-1}(t)"})," exists since ",i.a.createElement(b,{math:"\\Phi(t,t_1)^{-1}=\\Phi(t_1,t)"})," is known to exist. To verify the claim, we have",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\begin{aligned}\r \\dot{\\Phi}&=[F-GR^{-1}G^\\intercal P]\\Phi\\\\\r &=FX-GR^{-1}G^\\intercal Y=\\dot{X}\r\n\\end{aligned}\r\n\\end{aligned}"}),"and",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\begin{aligned}\r \\frac{d}{dt}[P\\Phi]&=\\dot{P}\\Phi+P\\dot{\\Phi}\\\\\r\n&=-(PF+F^\\intercal P-PGR^{-1}G^\\intercal P+Q)\\Phi +P(F\\Phi-GR^{-1}G^\\intercal Y)\\\\\r\n&=-QX -F^\\intercal Y=\\dot{Y}\r \\end{aligned}\r\n\\end{aligned}"}),"Therefore, the relations ",i.a.createElement(b,{math:"X(t)=\\Phi(t,t_1),\\quad Y(t)=P(t)\\Phi(t,t_1)"})," satisfy the augmented linear ODE.",i.a.createElement("p",{align:"right"},i.a.createElement(b,{math:"\\blacksquare"}))),i.a.createElement("div",{className:"remark"},i.a.createElement("b",null,"Remark: "),"Another scenario that might occur is when ",i.a.createElement(b,{math:"t_1\\rightarrow \\infty"})," and the matrix coefficients are varying. In this case, an approximation for ",i.a.createElement(b,{math:"P_\\infty"})," is possible to obtain by integration of the linear ODE. However, that approximation depends on the length ",i.a.createElement(b,{math:"t_1"}),"  Thus, the approximation becomes better as ",i.a.createElement(b,{math:"t_1"})," is increased."),i.a.createElement("h3",null,"Const. Coeff and Finite Horizon (",i.a.createElement(b,{math:"t_1<\\infty"}),")"),"For this scenario, consider",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n-\\dot{P}=P(t)F+F^\\intercal P(t)-P(t)GR^{-1}G^\\intercal P(t)+Q\r\n\\end{aligned}"}),"where ",i.a.createElement(b,{math:"F,G,R"})," and ",i.a.createElement(b,{math:"Q"})," are constant matrices. The respective augmented linear differential equation is",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\left[\r\n\\begin{matrix}\r\n\\dot{X}(t)\\\\\r\n\\dot{Y}(t)\r\n\\end{matrix}\r\n\\right]=\\left[\r\n\\begin{matrix}\r\nF & -GR^{-1}G^\\intercal \\\\\r\n-Q & -F^\\intercal\r\n\\end{matrix}\r\n\\right]\\left[\r\n\\begin{matrix}\r\nX(t)\\\\\r\nY(t)\r\n\\end{matrix}\r\n\\right],\\quad \\left[\r\n\\begin{matrix}\r\nX(t_1)\\\\\r\nY(t_1)\r\n\\end{matrix}\r\n\\right]=\\left[\r\n\\begin{matrix}\r\nI\\\\\r\nP_1\r\n\\end{matrix}\r\n\\right].\r\n\\end{aligned}"}),"As long as all coefficients are constant, the closed-form solution is",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\left[\r\n\\begin{matrix}\r\nX(t)\\\\\r\nY(t)\r\n\\end{matrix}\r\n\\right]=\\exp\\left(\\left[\r\n\\begin{matrix}\r\nF & -GR^{-1}G^\\intercal \\\\\r\n-Q & -F^\\intercal\r\n\\end{matrix}\r\n\\right](t_1-t)\\right)\\left[\r\n\\begin{matrix}\r\nX(t_1)\\\\\r\nY(t_1)\r\n\\end{matrix}\r\n\\right].\r\n\\end{aligned}"}),"Setting,",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\left[ \r\n\\begin{matrix}\r\n\\Phi_{11}(t) && \\Phi_{12}(t)\\\\\r\n\\Phi_{21}(t) && \\Phi_{22}(t)\r\n\\end{matrix}\r\n\\right]=\r\n\\exp\\left(\\left[\r\n\\begin{matrix}\r\nF & -GR^{-1}G^\\intercal \\\\\r\n-Q & -F^\\intercal\r\n\\end{matrix}\r\n\\right](t_1-t)\\right)\r\n\\end{aligned}"}),"then, the Riccati closed-form solution is",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\nP(t)=\\left[\\Phi_{21}(t)X(t_1)+\\Phi_{22}(t)Y(t_1)\\right]\\left[\\Phi_{11}(t)X(t_1)+\\Phi_{12}(t)Y(t_1)\\right]^{-1}.\r\n\\end{aligned}"}),i.a.createElement("h3",null,"Constant Coeff. and ",i.a.createElement(b,{math:"t_1\\rightarrow \\infty"})),"When ",i.a.createElement(b,{math:"t_1 \\rightarrow \\infty"})," with constant coefficients we obtain the so-called  ",i.a.createElement("i",null,"Algebraic Riccati Equation"),i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\nP_\\infty F+F^\\intercal P_\\infty-P_\\infty GR^{-1}G^\\intercal P_\\infty+Q=0.\r\n\\end{aligned}"}),i.a.createElement(E,{src:"/images/p_infinite.svg",width:"50%"}),"This is also known as the ",i.a.createElement("i",null,"Steady-state Solution")," of the Riccati Equation. It is implicitly assumed that the dynamic system associated with the Riccati equation is controllable and observable. The controllability assumption guaranteeing that the steady-state solution exists and the observability assumption assures that this solution is positive definite.",i.a.createElement("p",null),"There are plenty of different approaches in the literature to solve the Algebraic version of Riccati Equation. Here, we focus on only one method that consists in construct the augmented matrix",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r M=\\left[\r \\begin{matrix}\r F & -GR^{-1}G^\\intercal \\\\\r -Q & -F^\\intercal \r \\end{matrix}\r \\right].\r\n\\end{aligned}"}),"This ",i.a.createElement(b,{math:"M"})," matrix has the property that there is no pure imaginary eigenvalue and if ",i.a.createElement(b,{math:"\\lambda"})," is an eigenvalue  of ",i.a.createElement(b,{math:"M"}),"  so is ",i.a.createElement(b,{math:"-\\lambda"}),".  We then construct a matrix ",i.a.createElement(b,{math:"T"}),i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\nT=\\left[\r\n\\begin{matrix}\r\nT_{11} & T_{12} \\\\\r\nT_{21} & T_{22} \r\n\\end{matrix}\r\n\\right]\r\n\\end{aligned}"}),"such that",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r T^{-1}MT=\\left[\r \\begin{matrix}\r -\\Lambda & 0 \\\\\r 0 & \\Lambda \r \\end{matrix}\r \\right].\r\n\\end{aligned}"}),"Thus, the solution is simply given by",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r P_\\infty=T_{21}T_{11}^{-1}.\r\n\\end{aligned}"}),i.a.createElement("div",{className:"proof"},i.a.createElement("b",null,"Proof:")," ",i.a.createElement("br",null),"To check this solution, first notice that",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r MT=T\\left[\r \\begin{matrix}\r -\\Lambda & 0 \\\\\r 0 & \\Lambda \r \\end{matrix}\r \\right].\r\n\\end{aligned}"}),"This results in",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r FT_{11}-GR^{-1}G^\\intercal T_{21}&=-T_{11}\\Lambda,\\\\\r -QT_{11}-F^\\intercal T_{21}&=-T_{21}\\Lambda.\r\n\\end{aligned}"}),"Multiplying by ",i.a.createElement(b,{math:"T_{11}^{-1}"})," on the right side of both equations and by ",i.a.createElement(b,{math:"T_{21}T_{11}^{-1}"})," only in the first, we have",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r T_{21}T_{11}^{-1}F-T_{21}T_{11}^{-1}GR^{-1}G^\\intercal T_{21}T_{11}^{-1}=-T_{21}\\Lambda T_{11}^{-1}\r\n\\end{aligned}"}),"and",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r -Q-F^\\intercal T_{21}T_{11}^{-1}=-T_{21}\\Lambda T_{11}^{-1}.\r\n\\end{aligned}"}),"Therefore,",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r (T_{21}T_{11}^{-1})F+F^{\\intercal}(T_{21}T_{11}^{-1})-(T_{21}T_{11}^{-1})GR^{-1}G^\\intercal (T_{21}T_{11}^{-1})+Q=0.\r\n\\end{aligned}"}),i.a.createElement("p",{align:"right"},i.a.createElement(b,{math:"\\blacksquare"}))),i.a.createElement("h3",null,i.a.createElement(b,{math:"P(t)"})," based on ",i.a.createElement(b,{math:"P_\\infty"})),"As discussed in ",i.a.createElement("b",null,i.a.createElement("a",{href:"#Anderson1971"},"[Anderson1971][pg. 361]")),", if the steady-state solution is available in advance, we can establish a closed-form solution for the differential Riccati Equation for the entire horizon. In other words, we seek the expression ",i.a.createElement(b,{math:"P(t)"})," for",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r -\\dot{P}=PF+F^\\intercal P-PGR^{-1}G^\\intercal P +Q,\r\n\\end{aligned}"}),"given ",i.a.createElement(b,{math:"P_\\infty"})," such that",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r P_\\infty F+F^\\intercal P_\\infty-P_\\infty GR^{-1}G^\\intercal P_\\infty +Q=0.\r\n\\end{aligned}"}),"For this purpose, consider",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r P(t)=P_\\infty+Z^{-1}(t),\\quad  t\\le 0\r\n\\end{aligned}"}),"where ",i.a.createElement(b,{math:"Z(t)"})," is the solution of",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\dot{Z}(t)=[A-GR^{-1}G^\\intercal P_\\infty]Z(t)+Z(t)[A-GR^{-1}G^\\intercal P_\\infty]^\\intercal-GR^{-1}G^\\intercal\r\n\\end{aligned}"}),"which is a ",i.a.createElement("i",null,"Differential Lyapunov Equation"),". The solution for ",i.a.createElement(b,{math:"Z(t)"})," is",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r Z(t)=\\bar{Z}+e^{-\\tilde{A}t}[Z(0)-\\bar{Z}]e^{-\\tilde{A}^\\intercal t}\r\n\\end{aligned}"}),"where",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\tilde{A}&=A-GR^{-1}G^\\intercal P_\\infty\\\\\r 0&=\\tilde{A}\\bar{Z}+\\bar{Z}\\tilde{A}-GR^{-1}G^\\intercal\\\\\r Z(0)&=[P(0)-P_\\infty]^{-1}\r\n\\end{aligned}"}),"Thus, we can write the close-form solution for the Differential Riccati Equation as",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r P(t)=P_\\infty+[\\bar{Z}+e^{-\\tilde{A}t}[Z(0)-\\bar{Z}]e^{-\\tilde{A}^\\intercal t}]^{-1},\\quad t\\le 0\r\n\\end{aligned}"}),i.a.createElement("h2",null,"Analytical Solution of the Differential Lyapunov Equation"),"Consider the differential Lyapunov equation of the form",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\dot{X}(t)=A^\\intercal X(t)+X(t)A+Q,\\quad X(0)=X_0.\r\n\\end{aligned}"}),i.a.createElement("div",{className:"lemma"},i.a.createElement("b",null,"Lemma:"),i.a.createElement("br",null),"The analytical closed-form solution for ",i.a.createElement("i",null,"Differential Lyapunov Equation")," is given by",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r X(t)=\\bar{X}+e^{At}(X(0)-\\bar{X})e^{A^\\intercal t}.\r\n\\end{aligned}"}),"where ",i.a.createElement(b,{math:"\\bar{X}"})," is solution of the ",i.a.createElement("i",null,"Algebraic Lyapunov Equation"),i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r A\\bar{X}+\\bar{X}A^\\intercal -Q=0.\r\n\\end{aligned}"})),i.a.createElement("div",{className:"proof"},i.a.createElement("b",null,"Proof:"),i.a.createElement("br",null),"First, one can proof that if A is Hurwitz then the algebraic equation ",i.a.createElement("eqref",{eqref:"{eq-algebraic_lyapunov_negative}"})," has unique solution. Thus, we can rewrite the Diff. Lyapunov Equation as",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r \\frac{d}{dt}({X(t)-\\bar{X}})=A(X(t)-\\bar{X})+(X(t)-\\bar{X})A^\\intercal\r\n\\end{aligned}"}),"therefore,",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r X(t)=\\bar{X}+e^{At}(X(0)-\\bar{X})e^{A^\\intercal t}.\r\n\\end{aligned}"}),i.a.createElement("p",{align:"right"},i.a.createElement(b,{math:"\\blacksquare"}))),i.a.createElement("div",{className:"remark"},i.a.createElement("b",null,"Remark: "),"The solution of the ",i.a.createElement("i",null,"Algebraic Lyapunov Equation")," can be obtained employing the Kronecker Product",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r vec(\\bar{X})=\\left[A\\otimes A\\right]^{-1}vec(Q).\r\n\\end{aligned}"})),i.a.createElement("h3",null,"References:"),i.a.createElement("p",null,i.a.createElement("b",null,i.a.createElement("a",{href:"#",name:"Anderson1971"}),"[Anderson1971]")," B. D. O. Anderson and J. B. Moore, Linear optimal control. 1971.")),i.a.createElement("p",null),i.a.createElement(k.a.DiscussionEmbed,{shortname:"marofe-github-io",config:a}))},P=function(e){var a={url:"https://marofe.github.io/?p="+e.note.link,identifier:"note-"+e.note.link,title:e.title};return i.a.createElement("article",null,i.a.createElement(d.a,null,i.a.createElement("title",null,e.note.title," | Marofe"),i.a.createElement("meta",{name:"description",content:e.note.desc})),i.a.createElement("h1",null,e.title),i.a.createElement("p",null,e.desc),i.a.createElement("p",{align:"right"},"Last Update:  1 March, 2020."),i.a.createElement("div",null,i.a.createElement("p",null,"Particle Filter perform ",i.a.createElement("i",null,"Sequential Monte Carlo"),' (SMC) Estimation based on point mass "particles" representation of probabilities densities. The basic SMC ideas in the form of ',i.a.createElement("i",null,"Sequential Importance Sampling")," (SIS) had been introduced in statistics back in the 1950s. Although, the major contribution to the development of SMC method was the inclusion of ",i.a.createElement("i",null,"resampling step"),", which coupled with the rise of faster and cheap computers made the particle filters quite useful in practical problems."),i.a.createElement("p",null,"The following content is based on the book ",i.a.createElement("a",{href:"#ristic"},"ristic2003beyond"),"."),i.a.createElement("h2",null,"Monte Carlo Integration"),"Suppose we want to evaluate a multidimensional integral in the form",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r I=\\int g(x)dx,\r\n\\end{aligned}"}),"where ",i.a.createElement(b,{math:"g: \\mathbb{R}^n \\rightarrow \\mathbb{R}^m"})," is some complicated function that it is not possible to integrate analytically. Now, if we can factorize ",i.a.createElement(b,{math:"g(x)=f(x)\\pi(x)"})," in such a way that ",i.a.createElement(b,{math:"\\pi: \\mathbb{R}^n\\rightarrow \\mathbb{R}"})," is interpreted as a probability density satisfying",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r I=\\int f(x)\\pi(x)dx,\\quad \\pi(x)\\ge 0,\\quad \\int \\pi(x)dx = 1\r\n\\end{aligned}"}),"and assuming that it is possible to draw ",i.a.createElement(b,{math:"N>>1"})," samples ",i.a.createElement(b,{math:"\\{x^i\\}_{i=1}^N"})," distributed according to ",i.a.createElement(b,{math:"\\pi(x)"}),", thus,  the SMC provides an estimate of ",i.a.createElement(b,{math:"I"})," given by",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\hat{I}_N = \\sum_{i=1}^{N}\\frac{1}{N}f(x^i).\r\n\\end{aligned}"}),"In this case, if ",i.a.createElement(b,{math:"\\{x^i\\}"})," are independent samples then we can show that ",i.a.createElement(b,{math:"\\hat{I}_N"})," is an ",i.a.createElement("i",null,"unbiased")," estimate of ",i.a.createElement(b,{math:"I"}),i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\mathbb{E}\\{I\\}=\\mathbb{E}\\{\\hat{I}_N\\},\r\n\\end{aligned}"}),"and according to the ",i.a.createElement("i",null,"law of large numbers"),", ",i.a.createElement(b,{math:"\\hat{I}_N"})," will almost surely converge to ",i.a.createElement(b,{math:"I"})," as ",i.a.createElement(b,{math:"N\\rightarrow \\infty"}),". The error of this MC estimator is of order ",i.a.createElement(b,{math:"\\mathcal{O}(N^{-1/2})"}),", meaning that the rate of convergence is independent of the dimension ",i.a.createElement(b,{math:"n"}),". This useful and important property of MC integration is due to the choice of samples ",i.a.createElement(b,{math:"x^i"})," according to ",i.a.createElement(b,{math:"\\pi(x)"}),", as they automatically come from regions of the state space that are important for the integration.",i.a.createElement("p",null),"In the Bayesian Estimation context, density ",i.a.createElement(b,{math:"\\pi(x)"})," is usually the ",i.a.createElement("i",null,"posterior")," density. Unfortunately, in almost all cases, it is not possible to sample effectively from the posterior distribution. To overcome this problem, a possible solution is to apply the so-called ",i.a.createElement("i",null,"Importance Sampling Method"),".",i.a.createElement("h3",null,"Importante Sampling Method"),"Suppose we can generate only samples from a specific density ",i.a.createElement(b,{math:"q(x)"}),", which is similar to ",i.a.createElement(b,{math:"\\pi(x)"}),". Then a correct weighting of the samples might still make the MC estimation effective. The pdf ",i.a.createElement(b,{math:"q(x)"})," is referred to as the ",i.a.createElement("i",null,"importance")," or ",i.a.createElement("i",null,"proposal"),' density. Its "similarity" with ',i.a.createElement(b,{math:"\\pi(x)"})," can be expressed by the following condition",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\pi(x)>0 \\Rightarrow q(x)>0,\\quad \\forall x \\in \\mathbb{R}^n,\r\n\\end{aligned}"}),"which means that ",i.a.createElement(b,{math:"\\pi(x)"})," and ",i.a.createElement(b,{math:"q(x)"})," have the same support. If this condition is valid, then",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r I=\\int f(x)\\pi(x)dx = \\int f(x)\\frac{\\pi(x)}{q(x)}q(x)dx,\r\n\\end{aligned}"}),"provided that ",i.a.createElement(b,{math:"\\frac{\\pi(x)}{q(x)}"})," is ",i.a.createElement("i",null,"upper bounded"),". A MC estimate of ",i.a.createElement(b,{math:"I"})," is computed by generation ",i.a.createElement(b,{math:"N>>1"})," independents samples ",i.a.createElement(b,{math:"\\{x^i\\}"})," distributed according to ",i.a.createElement(b,{math:"q(x)"})," and forming the weighted sum",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\hat{I}_N=\\frac{1}{N}\\sum_{i=1}^N f(x^i)\\tilde{w}(x^i),\r\n\\end{aligned}"}),"where",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\tilde{w}(x^i)=\\frac{\\pi(x^i)}{q(x^i)},\r\n\\end{aligned}"}),"are the importance weights. If the normalization factor for the desired ",i.a.createElement(b,{math:"\\pi(x)"})," is unknown then we also need to perform normalization of the importance weights",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\hat{I}_N=\\frac{\\frac{1}{N}\\sum_{i=1}^N f(x^i)\\tilde{w}(x^i)}{\\frac{1}{N}\\sum_{i=1}^N \\tilde{w}(x^i)}=\\sum_{i=1}^N f(x^i)w(x^i),\r\n\\end{aligned}"}),"where",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r w(x^i)=\\frac{\\tilde{w}(x^i)}{\\sum_{j=1}^N\\tilde{w}(x^j)}.\r\n\\end{aligned}"}),i.a.createElement("h3",null,"Sequential Importance Sampling (SIS)"),"Importance Sampling is a general MC integration method. In another hand, the ",i.a.createElement("i",null,"Sequential Importance Sampling")," algorithm is a MC method that forms the basis for most sequential MC filters developed over the past decades.",i.a.createElement("p",null),"This sequential MC approach is known variously as ",i.a.createElement("i",null,"bootstrap filtering, the condensation algorithm, particle filters, interacting particles approximations and survival of the fittest"),". It is a technique for implementing recursive Bayesian Filters by MC simulations. The key idea is to represent the required posterior density function by a set of random samples with associated weights and compute estimates based on these samples and weights. As the number of samples becomes larger, the SIS filter approaches the optimal Bayesian Estimator.",i.a.createElement("p",null),"The joint posterior density at time ",i.a.createElement(b,{math:"k"})," can be approximated as follows",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\np(X_k|Z_k)\\approx \\sum_{i=1}^N w_k^i\\delta(X_k-X_k^i),\r\n\\end{aligned}"}),"where ",i.a.createElement(b,{math:"X_k=\\{x_0,x_1,\\ldots,x_k\\}"})," and ",i.a.createElement(b,{math:"Z_k=\\{z_0,z_1,\\ldots,z_k\\}"})," are the state path and measurement history, respectively. If the samples ",i.a.createElement(b,{math:"X_k^i"})," were drawn from an importance density ",i.a.createElement(b,{math:"q(X_k|Z_k)"})," then",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r w_k^i \\propto \\frac{p(X_k^i|Z_k)}{q(X_k^i|Z_k)}.\r\n\\end{aligned}"}),"Considering that the importance density is chosen to factorize such that",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r q(X_k|Z_k)=q(x_k|X_{k-1},z_k)q(X_{k-1}|Z_{k-1}),\r\n\\end{aligned}"}),"then we can obtain samples ",i.a.createElement(b,{math:"X_k^i \\sim q(X_k|Z_k)"})," by augmenting each of the existing samples ",i.a.createElement(b,{math:"X_{k-1}^i\\sim q(X_{k-1}|Z_{k-1})"})," with the new state ",i.a.createElement(b,{math:"x_k^i \\sim q(x_k|X_{k-1},z_k)"}),".",i.a.createElement("p",null),"To derive the weight update equation, the pdf ",i.a.createElement(b,{math:"p(X_k|Z_k)"})," is first factorized as",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r p(X_k|Z_k)&=p(X_k|z_k,Z_{k-1})\\\\\r &=\\frac{p(z_k|X_k,Z_{k-1})p(X_k|Z_{k-1})}{p(z_k|Z_{k-1})}\\\\\r &=\\frac{p(z_k|x_k,X_{k-1},Z_{k-1})p(x_k|X_{k-1},Z_{k-1})p(X_{k-1}|Z_{k-1})}{p(z_k|Z_{k-1})}\\\\\r &=\\frac{p(z_k|x_k,X_{k-1},Z_{k-1})p(x_k|x_{k-1},X_{k-2},Z_{k-1})p(X_{k-1}|Z_{k-1})}{p(z_k|Z_{k-1})}\\\\\r &=p(z_k|x_k)p(x_k|x_{k-1})\\frac{p(X_{k-1}|Z_{k-1})}{p(z_k|Z_{k-1})}.\r\n\\end{aligned}"}),"Thus,",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r p(X_k|Z_k)\\propto p(z_k|x_k)p(x_k|x_{k-1})P(X_{k-1}|Z_{k-1}).\r\n\\end{aligned}"}),"Substituting in ",i.a.createElement("a",{href:"#"},"eq:update_weight"),", we have",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r w_k^i \\propto \\frac{p(z_k|x_k^i)p(x_k^i|x_{k-1}^i)}{q(x_k^i|X_{k-1}^i,Z_k)}\\frac{p(X_{k-1}^i|Z_{k-1})}{q(X_{k-1}^i|Z_{k-1})},\r\n\\end{aligned}"}),"therefore, we conclude that",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r w_k^i\\propto w_{k-1}^i\\frac{p(z_k|x_k^i)p(x_k^i|x_{k-1}^i)}{q(x_k^i|X_{k-1}^i,Z_k)}.\r\n\\end{aligned}"}),"Now, if ",i.a.createElement(b,{math:"q(x_k|X_{k-1},Z_{k})=q(x_k|x_{k-1},z_k)"})," then",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\nw_k^i\\propto w_{k-1}^i\\frac{p(z_k|x_k^i)p(x_k^i|x_{k-1}^i)}{q(x_k^i|x_{k-1}^i,z_k)}.\r\n\\end{aligned}"}),"In this case, the posterior filtered density ",i.a.createElement(b,{math:"p(x_k|Z_k)"})," can be approximated as",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\np(x_k|Z_k)\\approx \\sum_{i=1}^n w_k^i\\delta(x_k-x_k^i).\r\n\\end{aligned}"}),"The Figure ",i.a.createElement("a",{href:"#"},"fig:pdf")," illustrate this approximation for a simple problem.",i.a.createElement(E,{src:"/images/pdf.svg",width:"50%"}),"Filtering via SIS thus consists of recursive propagation of importance weights ",i.a.createElement(b,{math:"w_k^i"})," and support points ",i.a.createElement(b,{math:"x_k^i"})," as each measurement is received sequentially. This simple and general algorithm forms the basis of most particle filters. However, the choice of the importance density ",i.a.createElement(b,{math:"q(x)"})," plays a crucial role in the design of this type of filter.",i.a.createElement("h3",null,"The Optimal Importance Density"),"The choice of importance density ",i.a.createElement(b,{math:"q(x_k|x_{k-1},z_k)"})," is one of the most critical issues in the design of a particle filter. The optimal importance density function that minimizes the variance of importance weights, conditioned upon ",i.a.createElement(b,{math:"x_{k-1}"})," and ",i.a.createElement(b,{math:"z_k"})," has been shown to be",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r q(x_k|x_{k-1},z_k)=p(x_k|x_{k-1},z_k).\r\n\\end{aligned}"}),"Substitution of ",i.a.createElement("a",{href:"#"},"eq:opt_q")," into ",i.a.createElement("a",{href:"#"},"eq:update2_weight")," yields",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\nw_k^i&\\propto w_{k-1}^i\\frac{p(z_k|x_k)p(x_k|x_{k-1}^i)}{p(x_k|x_{k-1}^i,z_k)}\\\\\r\n&\\propto w_{k-1}^i\\frac{p(z_k|x_k)p(x_k|x_{k-1}^i)p(z_k|x_{k-1}^i)}{p(z_k|x_k,x_{k-1}^i)p(x_k|x_{k-1}^i)}\\\\\r &\\propto w_{k-1}^ip(z_k|x_{k-1}^i),\r\n\\end{aligned}"}),"which states that importance weights at time ",i.a.createElement(b,{math:"k"})," can be computed (and resampling) before the particles even be propagated to time ",i.a.createElement(b,{math:"k"}),". In order to use the optimal importance function, one has to be able to (i) sample from ",i.a.createElement(b,{math:"p(x_k|x_{k-1}^i,z_k)"})," and (ii) evaluate",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r p(z_k|x_{k-1}^i)&=\\int p(z_k,x_k|x_{k-1}^i)dx_k\\\\\r &=\\int p(z_k|x_k)p(x_k|x_{k-1}^i)dx_k\r\n\\end{aligned}"}),"up to a normalizing constant. In the general case either these two may not be straightforward. However, there are some special cases where the use of optimal importance density is possible.",i.a.createElement("p",null),"The first case is when ",i.a.createElement(b,{math:"x_k"})," is a member of a finite set (e.g. jump-Markov linear systems). The second case is a class of models for which ",i.a.createElement(b,{math:"p(x_k|x_{k-1}^i,z_k)"})," is Gaussian.",i.a.createElement("h3",null,"Gaussian Optimal Importance Density"),"Consider the case where the state dynamics is nonlinear but the measurement equation is linear and all the random elements in the model are additive Gaussian",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\nx_k&=f_{k-1}(x_{k-1})+\\nu_{k-1}\\\\\r\nz_k&=H_kx_k+\\varepsilon_k\\\\\r\n\\nu_k\\sim &\\mathcal{N}(0,Q_k),\\quad \\varepsilon_k \\sim \\mathcal{N}(0,R_k)\r\n\\end{aligned}"}),"From the Bayes formula it follows that",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r p(x_k|x_{k-1},z_k)\\propto p(z_k|x_k)p(x_k|x_{k-1})\r\n\\end{aligned}"}),"as the measurement is linear and the random elements are Gaussian, ",i.a.createElement(b,{math:"p(x_k|x_{k-1},z_k)"})," will be the product of Gaussian distributions and therefore will result in another Gaussian. Besides, we can write",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\np(x_k|x_{k-1},z_k)p(z_k|x_{k-1})= p(z_k|x_k)p(x_k|x_{k-1})\r\n\\end{aligned}"}),"Taking the logarithm both sides yield",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r p(x_k|x_{k-1},z_k)&\\sim \\mathcal{N}(a_k,\\Sigma_k),\\\\\r p(z_k|x_{k-1})&\\sim \\mathcal{N}(b_k,S_k),\r\n\\end{aligned}"}),"with",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r a_k&=f_{k-1}(x_{k-1})+\\Sigma_kH_k^\\intercal R_k^{-1}(z_k-b_k),\\\\\r \\Sigma_k &= Q_{k-1}-Q_{k-1}H_k^\\intercal S_k^{-1}H_kQ_{k-1},\\\\\r S_k &= H_kQ_{k-1}H_k^\\intercal + R_k,\\\\\r b_k&=H_kf_{k-1}(x_{k-1}).\r\n\\end{aligned}"}),i.a.createElement("h3",null,"Degeneracy Problem and Resampling"),"Ideally, the importance density function should be the posterior density itself ",i.a.createElement(b,{math:"p(x_k|Z_k)"}),". However, as this cannot be achieved, the variance of the importance weights might increase over time. This leads to the ",i.a.createElement("i",null,"degeneracy phenomenon"),".",i.a.createElement("p",null),"In practical terms, this means that after a certain number of steps, all but one particle will have negligible normalized weight. The degeneracy is impossible to avoid in the SIS framework and, hence, it was a major stumbling block in the development of sequential MC methods. Because a large computational effort is devoted to updating particles whose contribution to the approximation of ",i.a.createElement(b,{math:"p(x_k|Z_k)"})," is almost zero.",i.a.createElement("p",null),"One measure of degeneracy of a SIS algorithm follows",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r N_{eff}=\\frac{1}{\\sum_{i=1}^N(w_k^i)^2}.\r\n\\end{aligned}"}),"It is straightforward to verify that ",i.a.createElement(b,{math:"1\\le N_{eff}\\le N"})," with the following extreme cases: (i) if the weights are uniform (i.e. ",i.a.createElement(b,{math:"w_k^i=\\frac{1}{N}"}),") then ",i.a.createElement(b,{math:"N_{eff}=N"}),"; and (ii) if ",i.a.createElement(b,{math:"\\exists j\\in \\{1,\\ldots,N\\}"})," such that ",i.a.createElement(b,{math:"w_k^j=1"})," and ",i.a.createElement(b,{math:"w_k^i=0"})," for all ",i.a.createElement(b,{math:"i\\neq j"}),", then ",i.a.createElement(b,{math:"N_{eff}=1"}),". Therefore, small ",i.a.createElement(b,{math:"N_{eff}"})," indicates a severe degeneracy and vice versa.",i.a.createElement("p",null),"To cope with this problem, we need to perform a resampling step whenever a significant degeneracy is observed (i.e. when ",i.a.createElement(b,{math:"N_{eff}"})," fall below some threshold ",i.a.createElement(b,{math:"N_{thr}"}),"). Resampling eliminates samples with low importance and multiplies ones with high importance.",i.a.createElement("p",null),"It involves a mapping of random measures ",i.a.createElement(b,{math:"\\{x_k^i,w_k^i\\}_{i=1}^N"})," into new ones ",i.a.createElement(b,{math:"\\{\\tilde{x}_k^i,\\frac{1}{N}\\}_{i=1}^N"})," with uniform weights. The new set of samples ",i.a.createElement(b,{math:"\\{\\tilde{x}_k^i\\}_{i=1}^N"})," is generated by resampling (with replacement) ",i.a.createElement(b,{math:"N"})," times from the approximate discrete representation of ",i.a.createElement(b,{math:"p(x_k|Z_k)"}),", so that ",i.a.createElement(b,{math:"P\\{\\tilde{x}_k^i=x_k^j\\}=w_k^j"}),". The resulting samples compose an i.i.d set and hence the new weights are uniform.",i.a.createElement("p",null),"One way to implement the resampling step is by the ",i.a.createElement("i",null,"Cumulative Sum of Weight Algorithm")," (CSW). This implementation consist of generating ",i.a.createElement(b,{math:"N"})," i.i.d variables from the uniform distribution ",i.a.createElement(b,{math:"\\mathcal{U}[0,1]"}),", sorting them in ascending order and comparing them with the cumulative sum of normalized weights. The Figure ",i.a.createElement("a",{href:"#"},"fig:csw")," illustrate this procedure.",i.a.createElement(E,{src:"/images/csw.svg",width:"50%"}),i.a.createElement("h2",null,"The Bootstrap Filter (SIR)"),"Proposed in ",i.a.createElement("a",{href:"#gordon"},"gordon1993novel"),". The ",i.a.createElement("i",null,"Sequential Importance Resampling")," (SIR) filter is derived from the SIS algorithm by choosing the importance density to be the ",i.a.createElement("i",null,"transitional prior")," and performing resampling every time step.",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r q(x_k|x_{k-1},z_k)=p(x_k|x_{k-1}).\r\n\\end{aligned}"}),"A sample ",i.a.createElement(b,{math:"x_k^i\\sim p(x_k|x_{k-1}^i)"})," can be generated first generating a process noise sample ",i.a.createElement(b,{math:"\\nu_{k-1}^i\\sim p(\\nu_k)"})," and setting ",i.a.createElement(b,{math:"x_k^i=f(x_{k-1}^i,\\nu_{k-1}^i)"}),". For this particular choice of importance density, the weight update is",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r w_k^i\\propto w_{k-1}^ip(z_k|x_k).\r\n\\end{aligned}"}),"However, as the resampling step is performed every iteration, we have ",i.a.createElement(b,{math:"w_{k-1}^i=\\frac{1}{N}"})," for all ",i.a.createElement(b,{math:"i=1,\\ldots,N"}),". Thus, the weight update simplifies to",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r w_k^i \\propto p(z_k|x_k).\r\n\\end{aligned}"}),i.a.createElement("h3",null,"Cons"),"As the importance sampling is independent of measurement ",i.a.createElement(b,{math:"z_k"}),", the state space is explored without any knowledge of the observations. Therefore, this filter is sensitive to outliers. Furthermore, as resampling is applied every iteration, this can result in rapid ",i.a.createElement("i",null,"loss of diversity")," in particles.",i.a.createElement("h3",null,"Pros"),"The SIR method has the advantage that the importance weights are easily evaluated and the importance density can be easily sampled.",i.a.createElement("h2",null,"Local Linearization Particle Filters (LLPF)"),"The optimal importance density can be approximated by incorporating the most current measurement ",i.a.createElement(b,{math:"z_k"})," via a bank of extended or unscented Kalman Filters. The idea is to use for each particle a separate EKF or UKF to generate and propagate a Gaussian importance distribution; that is,",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r q(x_k^i|x_{k-1}^i,z_k)=\\mathcal{N}(\\hat{x}_k^i,\\hat{P}_k^i),\r\n\\end{aligned}"}),"where ",i.a.createElement(b,{math:"\\hat{x}_k^i"})," and ",i.a.createElement(b,{math:"\\hat{P}_k^i"})," are estimates of the mean and covariance computed by EKF or UKF at time ",i.a.createElement(b,{math:"k"})," using measurement ",i.a.createElement(b,{math:"z_k"}),". We refer to the corresponding particle filter as the ",i.a.createElement("i",null,"Local Linearization Particle Filter")," (LLPF).",i.a.createElement("p",null),"The local linearization method for approximation of the importance density propagates the particles towards the likelihood function and consequently, the LLPF performs better than SIR filter. The additional computational cost of using such an importance density is often more than offset by a reduction in the number of samples required to achieve a certain level of performance.",i.a.createElement("div",{className:"remark"},i.a.createElement("b",null,"Remark: "),"Since Particle Filters are very expensive in terms of computational requirements, one should use them only in cases in which the conventional Kalman Filter does not produce satisfactory results."),i.a.createElement("h3",null,"References:"),i.a.createElement("p",null,i.a.createElement("a",{name:"gordon"}),"GORDON, N. J.; SALMOND, D. J.; SMITH, A. F. Novel approach to nonlinear/nongaussian bayesian state estimation. In: IET. IEE proceedings F (radar and signal processing). [S.l.], 1993. v. 140, n. 2, p. 107\u2013113"),i.a.createElement("p",null,i.a.createElement("a",{name:"ristic"}),"RISTIC, B.; ARULAMPALAM, S.; GORDON, N. Beyond the Kalman Filter: Particle Filters for Tracking Applications. [S.l.]: Artech House, 2003. ISBN 9781580538510.")),i.a.createElement(k.a.DiscussionEmbed,{shortname:"marofe-github-io",config:a}))},I=function(e){var a={url:"https://marofe.github.io/?p="+e.note.link,identifier:"note-"+e.note.link,title:e.title};return i.a.createElement("article",null,i.a.createElement(d.a,null,i.a.createElement("title",null,e.note.title," | Marofe"),i.a.createElement("meta",{name:"description",content:e.note.desc})),i.a.createElement("h1",null,e.title),i.a.createElement("p",null,e.desc),i.a.createElement("p",{align:"right"},"Last Update:  15 March, 2020."),i.a.createElement("div",null,i.a.createElement("h2",null,"Introduction"),"Consider available a collection of ",i.a.createElement(b,{math:"N"})," predictor-response samples ",i.a.createElement(b,{math:"\\{(x_i,y_i)\\}_{i=1}^N"})," where each ",i.a.createElement(b,{math:"x_i=(x_{i1},x_{1,i2},\\ldots,x_{ip})"})," is a p-dimensional vector of features or predictors, and ",i.a.createElement(b,{math:"y_i \\in \\mathbb{R}"})," is the associated response variable. In the linear regression setting, the goal is to approximate the response variable using a linear combination of the predictors",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r y_i=\\beta_0+\\sum_{j=1}^p x_{ij}\\beta_j ,\\quad i=1,\\ldots,N\r\n\\end{aligned}"}),"where the main problem is to estimate the model parameters ",i.a.createElement(b,{math:"\\beta=(\\beta_0,\\beta_1,\\ldots,\\beta_p)"}),". The usual ``least-square'' estimator is based on minimizing squared-error loss",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\min_{\\beta} \\frac{1}{2N}\\|y-X\\beta\\|_2^2\r\n\\end{aligned}"}),"where ",i.a.createElement(b,{math:"y=(y_1,y_2,\\ldots,y_N)"})," and",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r X=\\left[\r \\begin{matrix}\r 1 & x_{11} & x_{12} & \\cdots & x_{1p}\\\\\r 1 & x_{21} & x_{22} & \\cdots & x_{2p}\\\\\r \\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\r 1 & x_{N1} & x_{N2} & \\cdots & x_{Np}\\\\\r \\end{matrix}\r \\right]\r\n\\end{aligned}"}),"Typically, first a standardization of the predictors are made so that each column is centered (",i.a.createElement(b,{math:"\\frac{1}{N}\\sum_i x_{ij}=0"}),") and has unit variance (",i.a.createElement(b,{math:"\\frac{1}{N}\\sum_ix_{ij}^2=1"}),"). Without standardization the solutions would depend on the units. A simple way to standardize is computing the mean ",i.a.createElement(b,{math:"\\mu_x = \\sum_i x_{ij}"})," and variance ",i.a.createElement(b,{math:"\\sigma_x^2=\\frac{1}{N-1}\\sum_i (x_{ij}-\\mu)^2"})," and replace each predictor by",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\tilde{x}_{ij}=\\frac{x_{ij}-\\mu_x}{\\sigma_x}\r\n\\end{aligned}"}),"With the standardization the bias coefficient ",i.a.createElement(b,{math:"\\beta_0"})," can be omitted. It is usually convenient also to work with the outcome values ",i.a.createElement(b,{math:"y_i"})," centered (",i.a.createElement(b,{math:"\\frac{1}{N}\\sum_i y_i=0"}),"). However, there are, at least, two reasons why we might consider alternatives to the usual ``least-square''. The first reason is ",i.a.createElement("i",null,"accuracy"),". It is well-known that the LS estimator often has low bias but large variance. This can be improved by shrinking the values of the model parameters. Although, by doing so we introduce more bias but reduce the variance of the predicted values, and hence may improve the overall prediction accuracy assessed by the Mean Square Error (MSE). The second reason is ",i.a.createElement("i",null,"interpretation"),". It is much harder to interpret too many features, consequently, we often would like to identify a smaller subset of features that exhibit the strongest effects. The lasso provides an automatic way to simultaneously shrinking the coefficients and reduce the model complexity. In order to do so, a ",i.a.createElement(b,{math:"l_1"}),"-constraint is added to the usual least-square problem",i.a.createElement("a",{name:"problem1"}),i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\min_{\\beta} &\\frac{1}{2N}\\|y-X\\beta\\|_2^2\\\\\r\n\\text{s.t. } &\\|\\beta\\|_1 \\le t\r\n\\end{aligned}"}),"The bound ",i.a.createElement(b,{math:"t"})," in the lasso criterion controls the complexity of the model. A value ",i.a.createElement(b,{math:"t"})," too small can prevent the lasso from capturing the main signal in the data, while too large a value can lead to overfitting. The problem ",i.a.createElement("a",{href:"#problem1"},"problem1")," can be rewritten in an equivalent form so-called ",i.a.createElement("i",null,"Lagrangian form"),i.a.createElement("a",{name:"problem2"}),i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\min_{\\beta} \\frac{1}{2N}\\|y-X\\beta\\|_2^2+\\lambda \\|\\beta\\|_1 \r\n\\end{aligned}"}),"It is possible to demonstrate that there exist one-to-one correspondence between ",i.a.createElement("a",{href:"#problem1"},"problem1")," and ",i.a.createElement("a",{href:"#problem2"},"problem2")," for each ",i.a.createElement(b,{math:"t"})," chosen as bound ",i.a.createElement(b,{math:"\\|\\beta\\|_1\\le t"})," in ",i.a.createElement("a",{href:"#problem1"},"problem1")," there exist a corresponding value ",i.a.createElement(b,{math:"\\lambda"})," for ",i.a.createElement("a",{href:"#problem2"},"problem2")," that yields the same solution. Conversely, for an fixed ",i.a.createElement(b,{math:"\\lambda"}),", the solution ",i.a.createElement(b,{math:"\\hat{\\beta}_\\lambda"})," of the Lagrangian form solves the problem ",i.a.createElement("a",{href:"#problem1"},"problem1")," with ",i.a.createElement(b,{math:"t=\\|\\hat{\\beta}_\\lambda\\|_1"}),".",i.a.createElement("div",{className:"remark"},i.a.createElement("b",null,"Remark:"),i.a.createElement("br",null)," Note that in many descriptions of the lasso, the term ",i.a.createElement(b,{math:"1/2N"})," is often replaced by ",i.a.createElement(b,{math:"1/2"})," or ",i.a.createElement(b,{math:"1"}),". This makes no difference for ",i.a.createElement("a",{href:"#problem1"},"problem1")," or just mean a different value of ",i.a.createElement(b,{math:"\\lambda"})," at ",i.a.createElement("a",{href:"#problem2"},"problem2"),"; However, this kind of standardization makes ",i.a.createElement(b,{math:"\\lambda"})," values comparable for different sample sizes (useful for cross-validation)."),i.a.createElement("h2",null,"Lasso Solution"),"The theory of convex analysis tell us that the sufficient and necessary conditions for a solution to problem ",i.a.createElement("a",{href:"#problem2"},"problem2")," take the form",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r -\\frac{1}{N}X^\\intercal(y-X\\beta)+\\lambda s=0\r\n\\end{aligned}"}),"where ",i.a.createElement(b,{math:"s=(s_1,s_2,\\ldots,s_p)"})," represents the subgradient for the absolute function (",i.a.createElement(b,{math:"s_i=\\text{sign}(\\beta_i)"})," if ",i.a.createElement(b,{math:"\\beta_i\\neq 0"})," or ",i.a.createElement(b,{math:"s_i \\in [-1,1]"})," otherwise). In other words, the solutions ",i.a.createElement(b,{math:"\\hat{\\beta}"})," to problem ",i.a.createElement("a",{href:"#problem2"},"problem2")," are the same as ",i.a.createElement(b,{math:"(\\hat{\\beta},\\hat{s})"})," to ",i.a.createElement("a",{href:"#"},"subgradient_eq"),". Expressing a problem in subgradient form can be useful for designing algorithms for finding its solutions. However, the Lagrangian Form is specially convenient for numerical computation by a simple procedure known as ",i.a.createElement("i",null,"coordinate descent"),".",i.a.createElement("h3",null,"Scalar case: single predictor"),"Consider the following lasso problem",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\min_{\\beta} \\frac{1}{2N}\\sum_{i=1}^N(y_i-x_i\\beta)^2+\\lambda |\\beta| \r\n\\end{aligned}"}),"where ",i.a.createElement(b,{math:"\\beta \\in \\mathbb{R}"})," . The optimal solutions is given by",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\beta^* = \\left\\{\r\n\\begin{matrix}\r\n(x^\\intercal x)^{-1}(x^\\intercal y-\\lambda N), & \\text{ if } \\frac{1}{N}x^\\intercal y > \\lambda \\\\\r\n0, & \\text{ if } |\\frac{1}{N}x^\\intercal y| \\le \\lambda \\\\\r\n(x^\\intercal x)^{-1}(x^\\intercal y+\\lambda N), & \\text{ if } \\frac{1}{N}x^\\intercal y < -\\lambda \\\\\r\n\\end{matrix}\r\n\\right.\r\n\\end{aligned}"}),"with ",i.a.createElement(b,{math:"x=(x_1,x_2,\\ldots,x_N)"}),".",i.a.createElement("div",{className:"proof"},i.a.createElement("b",null,"Proof:"),i.a.createElement("br",null),"From ",i.a.createElement("a",{href:"#"},"problem_scalar"),", one has",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r J&= \\frac{1}{2N}\\sum_i (y_i^2-2y_ix_i\\beta+\\beta^2x_i^2)+\\lambda|\\beta|\\\\\r &=\\frac{1}{2N}y^\\intercal y - \\frac{1}{N}x^\\intercal y \\beta + \\frac{1}{2N}x^\\intercal x\\beta^2 +\\lambda|\\beta|\r\n\\end{aligned}"}),"Taking the sub-gradient",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r \\partial J =  - \\frac{1}{N}x^\\intercal y + \\frac{1}{N}x^\\intercal x\\beta +\\lambda\\mathcal{D}(\\beta)\r\n\\end{aligned}"}),"if ",i.a.createElement(b,{math:"\\beta^* >0"})," then ",i.a.createElement(b,{math:"\\beta^*=(x^\\intercal x)^{-1}(x^\\intercal y-\\lambda N)"}),". In another hand, if ",i.a.createElement(b,{math:"\\beta^*<0"})," then ",i.a.createElement(b,{math:"\\beta^*=(x^\\intercal x)^{-1}(x^\\intercal y+\\lambda N)"}),". Notice, thereafter, that ",i.a.createElement(b,{math:"|\\frac{1}{N}x^\\intercal y|\\le \\lambda"})," implies ",i.a.createElement(b,{math:"\\beta^*=0"}),".",i.a.createElement("p",{align:"right"},i.a.createElement(b,{math:"\\blacksquare"}))),"For the case when the estimator is standardized ",i.a.createElement(b,{math:"\\frac{1}{N}\\sum_i x_i = 0"})," and ",i.a.createElement(b,{math:"\\frac{1}{N}\\sum_i x_i^2=1"}),"), the optimal solution simplifies to",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\beta^* = \\left\\{\r\n\\begin{matrix}\r\n\\frac{1}{N}x^\\intercal y-\\lambda, & \\text{ if } \\frac{1}{N}x^\\intercal y > \\lambda \\\\\r\n0, & \\text{ if } |\\frac{1}{N}x^\\intercal y| \\le \\lambda \\\\\r\n\\frac{1}{N}x^\\intercal y+\\lambda, & \\text{ if } \\frac{1}{N}x^\\intercal y < -\\lambda \\\\\r\n\\end{matrix}\r\n\\right.\r\n\\end{aligned}"}),"The solution also can be rewritten in a short-form",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\beta^* = \\mathcal{S}_\\lambda \\left(\\frac{1}{N}x^\\intercal y\\right)\r\n\\end{aligned}"}),"where ",i.a.createElement(b,{math:"\\mathcal{S}_\\lambda"})," is the ",i.a.createElement("i",null,"soft-threshoulding operator"),i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\mathcal{S}_\\lambda(x)=\\text{sign}(x)(|x|-\\lambda)_+\r\n\\end{aligned}"}),"that translate the argument ",i.a.createElement(b,{math:"x"})," toward zeros by the amount ",i.a.createElement(b,{math:"\\lambda"})," and sets it to zero if ",i.a.createElement(b,{math:"|x|\\le \\lambda"})," and ",i.a.createElement(b,{math:"(t)_+"})," denotes the positive part of ",i.a.createElement(b,{math:"t"})," (equal to ",i.a.createElement(b,{math:"t"})," if ",i.a.createElement(b,{math:"t>0"})," or zero otherwise). For the case when the estimator are not standardized, the short-form becomes",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\beta^* = (x^\\intercal x)^{-1}\\mathcal{S}_{\\lambda N} \\left(x^\\intercal y\\right)\r\n\\end{aligned}"}),i.a.createElement("h3",null,"Multivariate case: multiple predictors"),"In order to solve the full lasso problem we can develop a simple coordinatewise scheme where we repeatedly cycle through the predictor in some fixed (but arbitrary) order where at the ",i.a.createElement(b,{math:"j^{th}"})," step, we update the coefficient ",i.a.createElement(b,{math:"\\beta_j"})," while holding fixed all other coefficients (",i.a.createElement(b,{math:"\\beta_k,k\\neq j"}),"). Writing the objective function as",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\frac{1}{2N}\\sum_{i=1}^N(y_i-\\sum_{k\\neq j} x_{ik}\\beta_k-x_{ij}\\beta_j)^2+\\lambda \\sum_{k\\neq j} |\\beta_k| + \\lambda |\\beta_j|\r\n\\end{aligned}"}),"we see that the solution for each ",i.a.createElement(b,{math:"\\beta_j"})," can be expressed succinctly in terms of the ",i.a.createElement("i",null,"partial residual"),i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r r_i^{(j)}=y_i-\\sum_{k\\neq j} x_{ik}\\hat{\\beta}_k\r\n\\end{aligned}"}),"In terms of this partial residual, the ",i.a.createElement(b,{math:"j^{th}"})," coefficient is updated as",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\hat{\\beta}_j=(x_j^\\intercal x_j)^{-1}\\mathcal{S}_{\\lambda N}\\left(x_j^\\intercal r^{(j)}\\right)\r\n\\end{aligned}"}),"where ",i.a.createElement(b,{math:"x_j=(x_{1j},x_{2j},\\ldots,x_{Nj})"})," and ",i.a.createElement(b,{math:"r^{(j)}=(r_1^{(j)},r_2^{(j)},\\ldots,r_N^{(j)})"}),". Or equivalently, in terms of the ",i.a.createElement("i",null,"full residual"),",",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\hat{\\beta}_j=\\mathcal{S}_\\lambda\\left(\\hat{\\beta}_j+\\frac{1}{N}x_j^\\intercal r\\right)\r\n\\end{aligned}"}),"with ",i.a.createElement(b,{math:"r_i=y_i-\\sum_j x_{ij}\\hat{\\beta}_j"}),". The algorithm just described corresponds to the method of ",i.a.createElement("i",null,"cyclical coordinate descent"),", which minimizes this convex objective along each coordinate at a time. It is important to note that some conditions are required, because there are instances, involving non-separable penalty functions, in which coordinate descent schemes can become ``jammed''. Besides, in practice, one is often interested in finding the lasso solution not just for a single fixed value of ",i.a.createElement(b,{math:"\\lambda"}),", but rather the entire path of solutions over a range of possible ",i.a.createElement(b,{math:"\\lambda"})," values. A reasonable method for doing so is to begin with a value of ",i.a.createElement(b,{math:"\\lambda"})," just large enough so that the only optimal solution is the all-zeros vector. This value is equal to ",i.a.createElement(b,{math:"\\lambda_{max} = \\max_j |\\frac{1}{N}x_j^\\intercal y|"}),". Then we decrease ",i.a.createElement(b,{math:"\\lambda"})," by a small amount and run coordinate descent until convergence. Decreasing ",i.a.createElement(b,{math:"\\lambda"})," again and using the previous solution as a ``warm start'', we then run coordinate descent until convergence. In this way we can efficiently compute the solutions over a grid of ",i.a.createElement(b,{math:"\\lambda"})," values. We refer to this method as ",i.a.createElement("i",null,"pathwise coordinate descent"),".",i.a.createElement("div",{className:"remark"},i.a.createElement("b",null,"Remark:"),i.a.createElement("br",null)," Coordinate descent is especially fast for the lasso because the coordinatewise minimizers are explicitly available. Secondly, it exploits the sparsity of the problem for large enough values of ",i.a.createElement(b,{math:"\\lambda"})," most coefficients will be zero and will not be moved from zero. Another more efficient method is the ",i.a.createElement("i",null,"Least Angle Regression")," (LARS) algorithm which is a homotopy method that constructs the piecewise linear path."),i.a.createElement("h3",null,"Special Case: orthogonal predictors"),"The coordinate minimization scheme takes an especially simple form if the predictors are orthogonal, meaning that",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\frac{1}{N}x_j^\\intercal x_k = 0,\\quad \\text{for each } j\\neq k.\r\n\\end{aligned}"}),"In this case, the update rule simplifies",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\frac{1}{N}x_j^\\intercal r^{(j)} = \\frac{1}{N}x_j^\\intercal y\r\n\\end{aligned}"}),"Thus, in the special case of an orthogonal design, the lasso has an explicit closed-form solution, and no iterations are required. Interesting connection is the ",i.a.createElement("i",null,"wavelet filtering"),". Since wavelet bases are orthogonal, wavelet filtering correspond to this special case of lasso.",i.a.createElement("h2",null,"Bayesian Interpretation of LASSO"),"Recall that the ridge regression estimator can be viewed as a Bayesian estimate of ",i.a.createElement(b,{math:"\\beta"})," when imposing a Gaussian prior. Similarly, the lasso regression estimator can be viewed as a Bayesian estimate when imposing a Laplacian (or double exponential) prior for each parameter",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r p(\\beta_j)=\\frac{1}{2}\\lambda \\exp (-\\lambda |\\beta_j|)\r\n\\end{aligned}"}),"with joint density",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r p(\\beta)=p(\\beta_1)\\cdots p(\\beta_p)=\\frac{1}{2\\tau}\\exp\\left(-\\frac{\\|\\beta\\|_1}{\\tau}\\right), \\quad \\text{ with } \\tau = \\frac{1}{\\lambda}\r\n\\end{aligned}"}),"Moreover, the lasso prior puts more mass close to zero and in the tails than the ridge prior. hence, the tendency of the lasso to produce either zero or large estimates as depicted in Figure ",i.a.createElement("a",{href:"#gassian_laplacian"},"fig:gaussian_laplacian"),i.a.createElement("a",{name:"gaussian_laplacian"}),i.a.createElement(E,{src:"/images/gaussian_laplacian.svg",width:"50%"}),i.a.createElement("i",null,"Obs: Notice that the Laplacian Distribution has a more fat tail than the Gaussian Distribution."),i.a.createElement("p",null),"The LASSO estimate then correspond to the mode of the posterior distribution given by the Bayes' Rule",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r p(\\beta|Y)=\\frac{p(Y|\\beta)p(\\beta)}{p(Y)}\r\n\\end{aligned}"}),"or in other words, the MAP estimator",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\beta = \\arg \\min_\\beta -\\log p(\\beta|Y).\r\n\\end{aligned}"}),i.a.createElement("div",{className:"remark"},i.a.createElement("b",null,"Remark:"),i.a.createElement("br",null)," The ``true Bayesian'' also puts a prior on the penalty parameter, giving rise to Bayesian LASSO regression. In addition, for high-dimensions, the Bayesian posterior need not concentrate on the ``true'' parameter. Even though, its mode is a good estimator of the regression parameter."),i.a.createElement("h2",null,"Moments of the LASSO"),"In contrast to ridge regression, there are no explicit expressions for the bias and variance of the lasso estimator, only approximations. However, as with the ridge estimator, the ",i.a.createElement("i",null,"trade-off")," between bias and variance still holds, e.g., the bias of the lasso estimator increases and the variance decreases in proportion to the lasso penalty parameter. In order to assess the moments of the LASSO, first, consider the following approximation",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\|Y-X\\beta\\|_2^2+\\lambda\\|\\beta\\|_1 \\approx \\|Y-X\\beta\\|_2^2+\\frac{\\lambda}{2}\\sum_{j=1}^m\\frac{1}{|\\hat{\\beta}(\\lambda)|}\\beta_j^2\r\n\\end{aligned}"}),"Optimization of this approximation gives a ``ridge approximation'' to the lasso",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\hat{\\beta}(\\lambda)\\approx [X^\\intercal X+\\lambda \\Psi(\\hat{\\beta}(\\lambda))]^{-1}X^\\intercal Y\r\n\\end{aligned}"}),"where ",i.a.createElement(b,{math:"\\Psi"})," is a diagonal matrix with ",i.a.createElement(b,{math:"\\Psi_{jj}=1/|\\hat{\\beta}_j(\\lambda)|"})," if ",i.a.createElement(b,{math:"\\hat{\\beta}_j(\\lambda)\\neq 0"})," and zero otherwise. Analogous to moment derivation of the ridge estimator, one obtains",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\mathbb{E}\\{\\hat{\\beta}(\\lambda)\\}\\approx [X^\\intercal X+\\lambda \\Psi(\\hat{\\beta}(\\lambda))]^{-1}X^\\intercal X\\beta \r\n\\end{aligned}"}),"and",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\text{Var}\\{\\hat{\\beta}(\\lambda)\\}\\approx \\sigma^2[X^\\intercal X+\\lambda \\Psi(\\hat{\\beta}(\\lambda))]^{-1}X^\\intercal X[X^\\intercal X+\\lambda \\Psi(\\hat{\\beta}(\\lambda))]^{-1}\r\n\\end{aligned}"}),i.a.createElement("h2",null,"Variations of LASSO"),i.a.createElement("h3",null,"Ordinary Lasso"),i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\|y-X\\beta\\|_2^2 + \\lambda \\|\\beta\\|_1\r\n\\end{aligned}"}),i.a.createElement("h3",null,"Elastic Net"),i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\|y-X\\beta\\|_2^2 + \\lambda (\\alpha\\|\\beta\\|_2^2+(1-\\alpha)\\|\\beta\\|_1)\r\n\\end{aligned}"}),i.a.createElement("h3",null,"Fused Lasso"),i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\n\\|y-X\\beta\\|_2^2 + \\lambda_1 \\|\\beta\\|_1+\\lambda_2 \\sum_{j=2}^p |\\beta_j-\\beta_{j-1}|\r\n\\end{aligned}"}),i.a.createElement("h2",null,"Lasso vs Robust Minimax"),"Consider the robust regression problem",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\min_{\\beta} \\max_{\\delta X \\in \\mathcal{U}} \\|y-(X+\\delta X)\\beta\\|_2\r\n\\end{aligned}"}),"where the uncertainty set ",i.a.createElement(b,{math:"\\mathcal{U}"})," is defined by",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\mathcal{U}=\\{(\\delta_1,\\delta_2,\\ldots,\\delta_m)| \\|\\delta_i\\|_2\\le c_i, i=1,2,\\ldots,m,\\delta_i \\in \\mathbb{R}^n\\}\r\n\\end{aligned}"}),i.a.createElement("div",{className:"lemma"},i.a.createElement("b",null,"Lemma:"),i.a.createElement("br",null),"The solution for this robust minimax problem is equivalent to",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\\min_{\\beta} \\|y-X\\beta\\|_2+\\sum_{i=1}^m c_i|\\beta_i|\r \\end{aligned}"})),i.a.createElement("div",{className:"proof"},i.a.createElement("b",null,"Proof:"),i.a.createElement("br",null),"Assume that ",i.a.createElement(b,{math:"\\beta"})," is given. Notice that",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\|y-(X+\\delta X)\\beta\\|_2&=\\|y-(X+(\\delta_1,\\delta_2,\\ldots,\\delta_m))\\beta\\|_2\\\\\r &=\\|y-X\\beta-\\sum_{i=1}^m \\delta_i \\beta_i\\|_2\\\\\r &\\le \\|y-X\\beta\\|_2-\\|\\sum_{i=1}^m \\delta_i \\beta_i\\|_2\\\\\r &\\le\\|y-X\\beta\\|_2-\\sum_{i=1}^m c_i |\\beta_i|\\\\\r \\end{aligned}"}),"Note that the maximum ",i.a.createElement(b,{math:"\\delta X^*=(\\delta_1^*,\\delta_2^*,\\ldots,\\delta_m^*)=(c_1,c_2,\\ldots,c_m)"}),". Now, consider a particular structure to the uncertainty ",i.a.createElement(b,{math:"\\delta_i"})," given by",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\delta_i&=\\left\\{\r \\begin{matrix}\r -c_iu\\text{sign}(\\beta_i), & \\text{ if }\\beta_i \\neq 0,\\\\\r -c_iu,& \\text{ if } \\beta_i=0.\r \\end{matrix}\r \\right.\\\\\r \\text{where}\\\\\r u&=\\left\\{\r \\begin{matrix}\r \\frac{y-X\\beta}{\\|y-X\\beta\\|_2}, & \\text{ if } y\\neq X\\beta,\\\\\r \\text{any unitary vector} & \\text{ otherwise.}\r \\end{matrix}\r \\right.\r \\end{aligned}"}),"Consequently, one has",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\max_{\\delta X \\in \\mathcal{U}} \\|y-(X+\\delta X)\\beta\\|_2&\\ge\\|y-(X+\\delta X^*)\\beta\\|_2\\\\\r &=\\|y-X\\beta+\\sum_{\\beta_i\\neq 0} c_iu\\text{sign}(\\beta_i)\\beta_i\\|_2\\\\\r &=\\|y-X\\beta+\\left(\\sum_{i} c_i|\\beta_i|\\right)u\\|_2\\\\\r &=\\|y-X\\beta\\|_2+\\sum_i c_i|\\beta_i|\r \\end{aligned}"}),"Finally, one concludes that",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r \\min_{\\beta} \\max_{\\delta X \\in \\mathcal{U}} \\|y-(X+\\delta X)\\beta\\|_2=\\min_{\\beta} \\|y-X\\beta\\|_2+\\sum_i c_i|\\beta_i|\r \\end{aligned}\r "}),i.a.createElement("p",{align:"right"},i.a.createElement(b,{math:"\\blacksquare"})))),i.a.createElement(k.a.DiscussionEmbed,{shortname:"marofe-github-io",config:a}))},j=function(e){var a={url:"https://marofe.github.io/?p="+e.note.link,identifier:"note-"+e.note.link,title:e.title};return i.a.createElement("article",null,i.a.createElement(d.a,null,i.a.createElement("title",null,e.note.title," | Marofe"),i.a.createElement("meta",{name:"description",content:e.note.desc})),i.a.createElement("h1",null,e.title),i.a.createElement("p",null,e.desc),i.a.createElement("p",{align:"right"},"Last Update:  26 August, 2020."),i.a.createElement("div",null,i.a.createElement("p",null,"The following content is based on the book ",i.a.createElement("a",{href:"#sarkka"},"SARKKA, 2013"),"."),i.a.createElement("h2",null,"Bayes Framework"),"In the Bayes Framework, all results are treated as being approximations to certain probability distributions or their parameters. Probability distributions are used both to represent uncertainties in the models and for modeling the physical randomness. ",i.a.createElement("p",null),"The term optimal filtering traditionally refers to a class of methods that can be used for estimating the state of a time-varying system which is indirectly observed through noisy measurements. The term optimal in this context refers to statistical optimality. Bayesian filtering refers to the Bayesian way of formulating optimal filtering. ",i.a.createElement("p",null),"In the case of state estimation of a dynamic system, the term state refers to the collection of dynamic variables which fully describe the system in a given instant of time.",i.a.createElement("p",null),"The noise in the measurements means that they are uncertain. Even if we knew the true system state, the measurements would not be deterministic functions of the state, but would have a distribution of possible values. The time evolution of the state is modeled as a dynamic system which is perturbed by a certain process noise. This noise is used for modeling the uncertainties in the system dynamics which can be a natural disturb that the system is facing or a poorly knowledge of the system behavior itself. In most cases, the system may not be truly stochastic, but stochasticity is used for representing the model uncertainties.",i.a.createElement("p",null),"Bayesian Smoothing is considered a class of methods within the field of Bayesian filtering. However, while Bayesian filters in their basic form only compute estimates of the current state of the system given the history of measurements, Bayesian smoothers can be used to reconstruct states that happened before  the current time.",i.a.createElement("p",null),"Phenomena which can be modeled as time-varying systems of the above type are very common in engineering applications. For example, in navigation, aerospace engineering, space engineering, remote surveillance, telecommunications, physics, audio signal processing, control engineering, finance, and many other fields. ",i.a.createElement("p",null),i.a.createElement("p",null),"In medicine, we can use the Bayesian framework to work with brain imaging methods such as electroencephalography (EEG), magnetoencephalography (MEG), parallel functional magnetic resonance imaging (fMRI) and many others.",i.a.createElement("p",null),"Another case is the estimation of spread of infectious diseases which often has uncertainties in the dynamic equation and measurements. Others dynamic process in biology such predator-prey models, population growth can also be modeled as stochastic differential equations and the state estimation problem also can be formulated as an optimal filtering and smoothing problem.",i.a.createElement("p",null),"In the same vein, learning systems or adaptive systems can often be mathematically formulated in terms of optimal filters and smoother as well, and they have a close relationship with Bayesian non-parametric modeling.",i.a.createElement("p",null),"In general, any physical system which is measured through non-ideal sensors can be formulated as stochastic state space models, and the time evolution of the system can be estimated using Bayesian filtering.",i.a.createElement("p",null),"The history of optimal filtering starts from the Wiener filter in 1950, which is a frequency domain solution to the problem of least squares optimal filtering of stationary Gaussian signals. The Wiener filter is still important in communication applications, digital signal processing and image processing. The disadvantage of the Wiener filter is that it can only be applied to stationary signals.",i.a.createElement("p",null),"The success of optimal linear filtering in engineering applications is mostly due to the seminal article of Kalman (1960), which describe the recursive solution to the optimal discrete-time (or sampled) linear filtering problem. One reason for the success is that the Kalman Filter can be understood and applied with very much lighter mathematical machinery than the Wiener filter. Besides, the Kalman filter also contains the Wiener filter as its limiting special case.",i.a.createElement("p",null),"In the early states of its history, the Kalman Filter was soon discovered to belong to the class of Bayesian filters. Although the original derivation of the Kalman Filter was based on the least squares approach, the same equations can be derived from pure probabilistic Bayesian analysis. The corresponding Bayesian smoothers were also developed soon after the invention of the Kalman Filter.",i.a.createElement("p",null),"In mathematical terms, optimal filtering and smoothing are considered to be statistical inversion problems where the unknown quantity is a vector valued time series ",i.a.createElement(b,{math:"\\{x_0,x_1,\\ldots\\}"})," which is observed through a set of noisy measurements ",i.a.createElement(b,{math:"\\{y_1,y_2,\\ldots\\}"}),". The purpose of the statistical inversion problem is to estimate the hidden states ",i.a.createElement(b,{math:"x_{0:T}=\\{x_0,x_1,\\ldots,x_T\\}"})," from the observed measurements set ",i.a.createElement(b,{math:"y_{1:T}=\\{y_1,y_2,\\ldots,y_T\\}"}),", which means that in the Bayesian sense we want to compute the joint posterior distribution of all the states given all the measurements. In principle, this can be done by straightforward application of Bayes' rule",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r p(x_{0:T}|y_{1:T})=\\frac{p(y_{1:T}|x_{0:T})p(x_{0:T})}{p(y_{1:T})},\r\n\\end{aligned}"}),"where ",i.a.createElement(b,{math:"p(x_{0:T})"})," is the prior distribution defined by the dynamic model, ",i.a.createElement(b,{math:"p(y_{1:T}|x_{0:T})"})," is the likelihood model for the measurements and ",i.a.createElement(b,{math:"p(y_{1:T})"})," is the normalization factor to ensure that ",i.a.createElement(b,{math:"\\int p(x_{0:T}|y_{1:T})dx_{0:T} = 1"}),". This factor can be computed by the total probability theorem",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r p(y_{1:T})=\\int p(y_{1:T}|x_{0:T})p(x_{0:T})dx_{0:T}.\r\n\\end{aligned}"}),"Unfortunately, this full posterior formulation has the serious disadvantage that each time we obtain a new measurement, the full posterior distribution would have to be recomputed. In other words, when the number of time steps increase, the dimensionality of the full posterior distribution also increases, which means that the computational complexity of a single time step increases. Thus eventually the computations will become intractable, no matter how much computational power is available. Without additional information or restrictive approximations, there is no way of getting over this problem in the full posterior computation.",i.a.createElement("p",null),"In order to solve this problem, we may relax this a bit in such a way that we can be satisfied with just a selected marginal distribution of the states. We also need to restrict the class of dynamic models to probabilistic Markov Sequences, which is not as restrictive as it may first seem.",i.a.createElement("p",null),"Usually, the following assumptions are made",i.a.createElement("ul",null,i.a.createElement("li",null,"An initial distribution specifies the prior probability distribution ",i.a.createElement(b,{math:"p(x_0)"})," of the hidden state ",i.a.createElement(b,{math:"x_0"})," at the initial time step ",i.a.createElement(b,{math:"k=0"}),"."),i.a.createElement("li",null," A dynamic model describes the system dynamics and its uncertainties as a Markov Sequence, defined in terms of the transition probability distribution ",i.a.createElement(b,{math:"p(x_k|x_{k-1})"}),"."),i.a.createElement("li",null,"A measurement model describes how the measurement ",i.a.createElement(b,{math:"y_k"})," depends on the current state ",i.a.createElement(b,{math:"x_k"}),". This dependence is modeled by specifying the conditional probability distribution of the measurement given the state, which is denoted as ",i.a.createElement(b,{math:"p(y_k|x_k)"}),".")),"Because computing the full joint distribution of the states at all times steps is computacionally very inefficient and unnecessary in real-time applications, in Bayesian filtering and smoothing the following marginal distributions are considered instead.",i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Filtering distributions")," computed by the Bayesian filter are the marginal distributions of the current state ",i.a.createElement(b,{math:"x_k"})," given the current and previous measurement ",i.a.createElement(b,{math:"y_{1:k}=\\{y_1,\\ldots,y_k\\}"}),":",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r p(x_k|y_{1:k}),\\quad k=1,\\ldots, T\r\n\\end{aligned}"})),i.a.createElement("li",null,i.a.createElement("b",null,"Prediction distributions")," which can be computed with the prediction step of the Bayesian filter are the marginal distributions of the future state ",i.a.createElement(b,{math:"x_{k+n}"}),", ",i.a.createElement(b,{math:"n"})," steps after the current time step:",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r p(x_{k+n}|y_{1:k}),\\quad k=1,\\ldots, T, \\quad n = 1,2,\\ldots\r\n\\end{aligned}"})),i.a.createElement("li",null,i.a.createElement("b",null,"Smoothing Distributions")," computed by the Bayesian smoother are the marginal distributions of the state ",i.a.createElement(b,{math:"x_k"})," given a certain interval ",i.a.createElement(b,{math:"y_{1:T}=\\{y_1,\\ldots,y_T\\}"})," of measurements with ",i.a.createElement(b,{math:"T>k"}),":",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r p(x_k|y_{1:T}), \\quad k=1,\\ldots , T.\r\n\\end{aligned}"}))),"Computing the filtering, prediction, and smoothing distributions require only a constant number of computations per time step, and thus the problem of processing arbitrarily long time series is solved.",i.a.createElement("p",null),"The well-known ",i.a.createElement("i",null,"Kalman Filter")," (KF) is a closed form solution to the linear Gaussian filtering problem.  The ",i.a.createElement("i",null,"Rauch-Tung-Striebel")," smoother (RTS) is the corresponding closed form smoother for linear Gaussian state space models. ",i.a.createElement("i",null,"Grid Filters")," and ",i.a.createElement("i",null,"Grid smoothers")," are solutions to Markov models with finite state spaces.",i.a.createElement("p",null),"But because the Bayesian optimal filtering and smoothing equations are generally computationally intractable, many kinds of numerical approximations methods have been developed, for example:",i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("i",null,"The extended Kalman Filter")," (EKF) approximates the non-linear and non-Gaussian measurements and dynamic models by first order Taylor expansion at the nominal solution. This results in a Gaussian approximation to the filtering distributions."),i.a.createElement("p",null),i.a.createElement("li",null," ",i.a.createElement("i",null,"The extended Rauch-Tung-Striebel smoother")," is the approximate non-linear smoothing algorithm corresponding to EKF."),i.a.createElement("p",null),i.a.createElement("li",null," ",i.a.createElement("i",null,"The unscentend Kalman Filter")," (UKF) approximates the propagation of densities through the non-linearities of measurement and noise processes using the ",i.a.createElement("i",null,"unscented transform"),". This also results in a Gaussian approximation."),i.a.createElement("p",null),i.a.createElement("li",null," ",i.a.createElement("i",null,"The unscented Rauch-Tung-Striebel smoother")," is the approximate non-linear smoothing algorithm corresponding to UKF."),i.a.createElement("p",null),i.a.createElement("li",null," ",i.a.createElement("i",null,"Sequential Monte Carlo methods")," or ",i.a.createElement("i",null,"particle filters (PF) and smoothers")," represent the posterior distribution as a weighted set of Monte Carlo samples."),i.a.createElement("p",null),i.a.createElement("li",null," ",i.a.createElement("i",null,"The unscented particle filter")," (UPF) or ",i.a.createElement("i",null,"local linearization based particle filter")," (LLPF) filtering methods that use UKF and EKF, respectively, for approximating the optimal importance distributions in particle filter settings."),i.a.createElement("p",null),i.a.createElement("li",null," ",i.a.createElement("i",null,"Rao-Blackwellized particle filters and smoothers")," use closed form integration (e.g., Kalman Filter and RTS smoothers) for some of the state variables and Monte Carlo integration for others.")),"Other methods also exist, for example, based on Gaussian mixtures.",i.a.createElement("h2",null,"Bayesian filtering equations and exact solutions"),"Bayesian filtering is considered with state estimation in general probabilistic state space models which have the following form",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r x_k&\\sim p(x_k|x_{k-1})\\\\\r y_k&\\sim p(y_k|x_k)\\\\\r k&=1,2,\\ldots\r\n\\end{aligned}"}),"where ",i.a.createElement(b,{math:"x_k \\in \\mathbb{R}^n"})," is the state of the system at time step ",i.a.createElement(b,{math:"k"}),", ",i.a.createElement(b,{math:"y_k \\in \\mathbb{R}^m"})," is the measurement at time step ",i.a.createElement(b,{math:"k"}),", ",i.a.createElement(b,{math:"p(x_k|x_{k-1})"})," is the dynamic model which describes the stochastic dynamic of the system, and ",i.a.createElement(b,{math:"p(y_k|x_k)"})," is the measurement model, which is the distribution of measurements given the state. The model is assumed to be Markovian, which means that it has the following properties",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\np(x_k|x_{0:k-1},y_{1:k-1})&=p(x_k|x_{k-1})\\\\\r\np(x_{k-1}|x_{k:T},y_{k:T})&=p(x_{k-1}|x_k)\r\n\\end{aligned}"}),"Another assumption usually made is the ",i.a.createElement("i",null,"Conditional independence of measurements")," that means",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r p(y_k|x_{1:k},y_{1:k-1})=p(y_k|x_k).\r\n\\end{aligned}"}),"With the Markovian assumption and conditional independence of measurements, the ",i.a.createElement("i",null,"joint prior distribution")," of the states and the ",i.a.createElement("i",null,"joint likelihood")," of the measurements are, respectively",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r p(x_{0:T})&=p(x_0)\\prod_{k=1}^Tp(x_k|x_{k-1})\\\\\r p(y_{1:T})&=\\prod_{k=1}^T p(y_k|x_k)\r\n\\end{aligned}"}),"In principle, for a given ",i.a.createElement(b,{math:"T"}),", we could simply compute the posterior distribution of the states by ",i.a.createElement("i",null,"Bayes' rule"),". However, this is intractable for most applications because the number of computations increases as new observations arrive. To cope with real problem we need to have some recursive algorithm that does a constant number of operations independent of the number of observations. For this reason, we shall not consider the full posterior computation at all, but concentrate on the above mentioned distributions: filtering and prediction distributions and corresponding smoothing distributions.",i.a.createElement("h3",null,"Bayesian filtering"),"The purpose of Bayesian filtering is to compute the marginal posterior distribution or filtering distribution of the state ",i.a.createElement(b,{math:"x_k"})," at each time step ",i.a.createElement(b,{math:"k"})," given the history of measurements up to the time step ",i.a.createElement(b,{math:"k"}),":",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r p(x_k|y_{1:k})\r\n\\end{aligned}"}),i.a.createElement("div",{className:"lemma"},i.a.createElement("b",null,"Lemma:"),i.a.createElement("br",null),"The recursive equations for the Bayesian filter are given by the following equations",i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("b",null,"Initialization:")," The recursion starts from the prior distribution ",i.a.createElement(b,{math:"p(x_0)"}),";"),i.a.createElement("li",null,i.a.createElement("b",null,"Prediction step:")," The predictive distribution fo the state ",i.a.createElement(b,{math:"x_k"})," at the time step ",i.a.createElement(b,{math:"k"}),", given the dynamic model, can be computed by the ",i.a.createElement("i",null,"Chapman-Kolmogorov equation"),i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r p(x_k|y_{1:k-1})=\\int p(x_k|x_{k-1})p(x_{k-1}|y_{1:k-1})dx_{k-1}.\r\n\\end{aligned}"})),i.a.createElement("li",null," ",i.a.createElement("b",null,"Update step:")," Given the measurement ",i.a.createElement(b,{math:"y_k"})," at time step ",i.a.createElement(b,{math:"k"})," the posterior distribution of the state ",i.a.createElement(b,{math:"x_k"})," can be computed by ",i.a.createElement("i",null,"Bayes' rule"),i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r p(x_k|y_{1:k})=\\frac{1}{c}p(y_k|x_k)p(x_k|y_{1:k-1}),\r\n\\end{aligned}"}),"where the constant ",i.a.createElement(b,{math:"c"})," is the normalization factor given by",i.a.createElement(p.BlockMath,{math:"\t\\begin{aligned}\r c=\\int p(y_k|x_k)p(x_k|y_{1:k-1})dx_k.\r\n\\end{aligned}"})))),i.a.createElement("div",{className:"proof"},i.a.createElement("b",null,"Proof:"),i.a.createElement("br",null),"Based on the Markovian and Conditioned Independence of measurement assumptions, one has",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\np(x_k,x_{0:k-1}|y_{1:k})&=p(x_k,x_{k-1}|y_k,y_{1:k-1})\\\\\r\n&=\\frac{p(y_k|x_k,x_{k-1},y_{1:k-1})p(x_k,x_{k-1}|y_{1:k-1})}{p(y_k|y_{1:k-1})}\\\\\r\n&=\\frac{p(y_k|x_k,x_{k-1},y_{1:k-1})p(x_k|x_{k-1},y_{1:k-1})p(x_{k-1}|y_{1:k-1})}{p(y_k|y_{1:k-1})}\\\\\r\n&=\\frac{p(y_k|x_k)p(x_k|x_{k-1})p(x_{k-1}|y_{1:k-1})}{p(y_k|y_{1:k-1})}.\r\n\\end{aligned}"}),"Therefore,",i.a.createElement(p.BlockMath,{math:"\\begin{aligned}\r\np(x_k|y_{1:k})&=\\int p(x_k,x_{k-1}|y_{1:k})dx_{k-1}\\\\\r\n&=\\int \\frac{p(y_k|x_k)p(x_k|x_{k-1})p(x_{k-1}|y_{1:k-1})}{p(y_k|y_{1:k-1})} dx_{k-1}\\\\\r\n&=\\frac{p(y_k|x_k)\\int p(x_k|x_{k-1})p(x_{k-1}|y_{1:k-1}) dx_{k-1}}{p(y_k|y_{1:k-1})}\\\\\r\n&=\\frac{p(y_k|x_k)p(x_k|y_{1:k-1})}{\\int p(y_k,x_k|y_{1:k-1})dx_k}\\\\\r\n&=\\frac{1}{c}p(y_k|x_k)p(x_k|y_{1:k-1}).\r\n\\end{aligned}"}),i.a.createElement("p",{align:"right"},i.a.createElement(b,{math:"\\blacksquare"}))),i.a.createElement("h2",null,"References:"),i.a.createElement("p",null,i.a.createElement("a",{name:"sarkka"}),"S\xe4rkk\xe4, S., 2013. Bayesian filtering and smoothing. Cambridge University Press.")),i.a.createElement(k.a.DiscussionEmbed,{shortname:"marofe-github-io",config:a}))},R=function(e){var a;switch(e.note.class){case"EkfLie":a=i.a.createElement(q,{note:e.note});break;case"RiccatiEq":a=i.a.createElement(N,{note:e.note});break;case"particleFilter":a=i.a.createElement(P,{note:e.note});break;case"Lasso":a=i.a.createElement(I,{note:e.note});break;case"BayesianFiltering":a=i.a.createElement(j,{note:e.note})}return i.a.createElement("div",{className:"divNote"},i.a.createElement("div",{className:"top"},i.a.createElement("h1",null,e.note.title),i.a.createElement("p",null,e.note.desc)),a)},S=function(e){function a(){var e,t;Object(w.a)(this,a);for(var n=arguments.length,r=new Array(n),l=0;l<n;l++)r[l]=arguments[l];return(t=Object(M.a)(this,(e=Object(T.a)(a)).call.apply(e,[this].concat(r)))).states={notes:[{id:0,title:"Extend Kalman Filter on Lie Groups",desc:"Here I provide the main equations for implementation of Extend Kalman Filter on Lie Groups.",link:"ekf-lie-groups",class:"EkfLie"},{id:1,title:"Analytical Solution of Riccati Equations",desc:"The Differential Riccati Equations are essential to solving many problems in optimal control and filtering. In this note, the analytical solution is discussed for both finite and infinite horizons.",link:"riccati-equation",class:"RiccatiEq"},{id:2,title:"Particle Filter and Monte Carlo Integration",desc:'Particle Filter perform Sequential Monte Carlo (SMC) Estimation based on point mass "particles" representation of probabilities densities. The basic SMC ideas in the form of Sequential Importance Sampling had been introduced in statistics back in the 1950s. In this note, an overview of this method is provided.',link:"particle-filter",class:"particleFilter"},{id:3,title:"LASSO Regression",desc:"This note is dedicated to summarize the main fundamentals of the lasso estimator. This method combines the usual least-square loss with a l1-constraint, or bound in the sum of the absolute values of the model parameters. Compared to the classical least-square, the lasso estimator has the effect of shrinking the regression coefficients or even setting some to zero. In this way, the lasso provides an automatic way to feature selection, and unlike another methods, the resulting optimization problem is convex, and can be solved efficiently for large problems. The lasso was proposed by Robert Tibshirani in 1996.",link:"lasso",class:"Lasso"},{id:4,title:"Bayesian Filtering",desc:"Bayesian Filtering methods are used to produce an accurate estimate of the state of a time-varying system based on multiple observational inputs (data). Interest in these methods has exploded in recent years, with numerous applications emerging in fields such as navigation, aerospace engineering, telecommunications, and medicine. The Bayesian approach to the estimation and filtering problem is far from new. It was pioneered by Stratonovich in the 1950s and 1960s - even before Kalman's seminal article in 1960.",link:"bayesian-filtering",class:"BayesianFiltering"}]},t.routes=t.states.notes.map(function(e){return i.a.createElement(m.a,{path:"/notes/"+e.link,render:function(a){return i.a.createElement(R,{note:e})}})}),t}return Object(F.a)(a,e),Object(A.a)(a,[{key:"render",value:function(){var e=this;return i.a.createElement("div",{className:"divPage"},i.a.createElement(m.a,{path:"/notes",exact:!0,render:function(a){return i.a.createElement(B,{notes:e.states.notes})}}),this.routes)}}]),a}(n.Component),z=function(e){return i.a.createElement("header",{className:"fix"},e.children)},X=function(){return i.a.createElement("div",{className:"divnav"},i.a.createElement("nav",null,i.a.createElement("ul",null,i.a.createElement("li",null,i.a.createElement("a",{href:"/"},"Home")),i.a.createElement("li",null,i.a.createElement("a",{href:"/tutorials"},"Tutorials")),i.a.createElement("li",null,i.a.createElement("a",{href:"/notes"},"Notes")),i.a.createElement("li",null,i.a.createElement("a",{href:"/publications"},"Publications")),i.a.createElement("li",null,i.a.createElement("img",{className:"brazilFlag",src:"https://upload.wikimedia.org/wikipedia/commons/thumb/0/05/Flag_of_Brazil.svg/2000px-Flag_of_Brazil.svg.png",alt:"Brazil's flag"})))))},C=function(e){var a=(new Date).getFullYear();return i.a.createElement("div",{className:"bottom"},a," - \xa9 Copyright - All rights reserved.")};var G=function(){return i.a.createElement(o,null,i.a.createElement(z,null,i.a.createElement("div",{className:"divHeader"},i.a.createElement("h2",null,"Marcos R. Fernandes")),i.a.createElement(X,null)),i.a.createElement(s.a,{basename:""},i.a.createElement(m.a,{path:"/",exact:!0,component:h}),i.a.createElement(m.a,{path:"/publications",component:u}),i.a.createElement(m.a,{path:"/tutorials",component:v}),i.a.createElement(m.a,{path:"/notes",component:S})),i.a.createElement(C,null))};Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));l.a.render(i.a.createElement(G,null),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then(function(e){e.unregister()})}},[[32,1,2]]]);
//# sourceMappingURL=main.b1167615.chunk.js.map